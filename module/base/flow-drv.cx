//-*-C-*-
/*******************************************************************************
*   ___   publicplace
*  ¦OUX¦  C+
*  ¦/C+¦  component
*   ---   base
*         flow driver
* ©overcq                on ‟Gentoo Linux 13.0” “x86_64”             2015‒1‒26
*******************************************************************************/
// Programista ‟oux” rozumie następujące zagadnienia podstawowe:
// ‣ przełączanie ‹zadań› wykonywane w pętli dla kolejnych obudzonych jako ograniczoną liniowym czasem aktywację kolejnych warstw wykonawczej sieci informacyjnej (jak biologicznej sieci neuronowo‐biochemicznej, inaczej elektryczno‐molekularnej, inaczej synaptyczno‐organicznej)
// ‣ informacyjno‐wykonawczą nienazywającą stwórczość bytową (molekularną) w postaci bezwarunkowej pętli programowej odpowiednio użytej
// ‣ pełną dynamiczność systemu ‘uidów’ ‹raportów›/‹impulsatorów›, która jest odrębna od używanego systemu zapewniania unikalności ‘idów’ zarejestrowanych ‹raportów›/‹impulsatorów› używanych pomiędzy ‹zadaniami›— jak rzeczywistość wykonawcza nie jest zależna od nazw, ale została oparta na statycznych nazwach, bo tutaj program jest kompilowany całościowo, więc nie potrzeba uzgadniać ‘uidu’ ‹raportu›/‹impulsatora›
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Przeprojektowywane pod rzeczywiste potrzeby.
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// [ta linia– do aktualizacji.] Instrukcje mogące przełączyć ‹zadanie›: “X_B”, [“X_E”], “Y_B”, “I_B”, “Da_W”. “D_M” i “D_W” zawsze przełączają jakby proceduralnie do ‹zadania› uruchamianego/zwalnianego, wykonują blok startowy/końcowy i wracają, więc nie zaliczają się do tego zbioru.
// Bloki startowe i końcowe ‹zadań› służą obowiązkowo do rejestracji (np. ‹raportów›) i zwalniania zasobów ‘interfejsu’ bytu ‹zadania› ·bezwarunkowo· widzianych przez inne. W blokach tych nie wolno przełączać (tzn. jawnie, nie przez normalne wykonywanie się “D_W”) ani wykonywać celu ‹zadania›. (Jeśli w bloku startowym zostanie użyta jedna z instrukcji przełączenia, to ‹zadanie› pozostanie zablokowane dla ‘schedulera’ w nie wznawialnym (nie obsługiwanym) stanie oczekiwania (“E_flow_Z_run_state_S_starting_by_task”). Jeśli zostanie użyta w bloku końcowym, to ‹zadanie› zostanie zablokowane (“E_flow_Z_run_state_S_stopping_by_task”) i usunięte przez ‹zadanie› usuwające, po przełączeniu do niego przez ‘schedulera’— bez wykonania reszty programu ‹zadania› usuwanego.)
// Adresy danych dostępnych przez identyfikatory (adresy ‹okien wglądu›) muszą być ponownie odczytane przed użyciem po możliwym przełączeniu ‹zadania›.
//------------------------------------------------------------------------------
// Zasadą jest, że instrukcje oczekiwania na coś przez ‹zadanie› odblokowują je dokładnie po zaistnieniu tego czegoś, bez zależności raportowania zaistnienia czegokolwiek, co ‹zadanie› ma zarejestrowane, ale czeka na to inną instrukcją przełączającą ‹zadanie›. Więc oczekiwanie sekwencyjne na różne rzeczy przez ‹zadanie› jest sytuacją obsługiwaną (tak jak powyżej napisane), aczkolwiek nie polecaną.
// Właścicielem ‹raportu›/‹impulsatora› jest ‹zadanie›, które na niego czeka (rejestracja oczekiwania– “X_M”/“Yi_M”), i tylko to ‹zadanie› może czekać na ten ‹raport›/‹impulsator› (w dowolnej liczbie miejsc wewnątrz własnej procedury); ale w ‘kompilacji’ ‹modułów› do “bibliotek” ‹zadanie› w ‹module› “main” —czyli w głównym pliku wykonywalnym programu— nie jest generatorem ‘uidu’ ‹raportu›/‹impulsatora›, który emituje któryś ‹moduł› “biblioteczny”. Właścicielem ‚raportów znacznikowych’ —statycznych, nierejestrowanych, pochodzących z “sygnałów” ‘uniksowych’— jest ‹zadanie› “main” (a procedura), ponieważ tylko to ‹zadanie› może obsługiwać stany programu uruchomionego przez nie w składnikach, a niektórych ‚raportów znacznikowych’– ‚scheduler’, ponieważ tylko on może zarządzać systemowym przepływem wykonania i go przełączać.
//------------------------------------------------------------------------------
// “process_call” – synchroniczne w przepływie pojedynczego ‹zadania› — funkcje żądane przy użyciu ‘uniksowego’ “sygnału”:
// Tylko jedno ‹zadanie› w programie (‹sterownik› funkcji programu zewnętrznego) może implementować żądania funkcji konkretnego (jednego) programu ‟oux” uruchomionego w systemie operacyjnym. Obecnie w programie ‟current-system-integrator” rozróżnianie programów jest realizowane przez rozróżnianie nazw procesów, tzn. nie jest uwzględniony przypadek, gdy program ‟oux” posiada procesy podrzędne. [To zdanie sprawdzić, bo coś już było zrobione.] Ponadto nie zostało (dla optymalizacji) zaimplementowane rozpoznawanie programu typu ‟oux” (uzupełnić o autoryzację potwierdzaną chwilowo?) oraz własnego (wykluczać u źródła), a wykonanie żądania funkcji do takiego programu nie jest dozwolone.
//TODO Rozwiązać inaczej “subid” ‹zadań› w wątku systemowym?
_internal
I _X_var( flow, call_req );
_internal
I _X_var( io, stream_write );
//==============================================================================
enum E_flow_Z_run_state
{ E_flow_Z_run_state_S_ready
, E_flow_Z_run_state_S_waiting_for_report
, E_flow_Z_run_state_S_waiting_for_timer
, E_flow_Z_run_state_S_waiting_for_call_reply
    #ifdef E_flow_C_itimer_system_unblock_report
, E_flow_Z_run_state_S_waiting_for_system_unblock_report
    #endif
, E_flow_Z_run_state_S_stopping_by_task
};
struct E_flow_Q_task_Z
{ Pc exe_stack;
  Z_clock_time run_state_time;
  Pc stack;
  Pc touched_stack;
    #ifdef C_line_report
  Pc proc_name;
    #endif
  N stack_size;
    #if defined( E_flow_C_thread_system_unblock_reports ) || defined( C_pthreads )
  pthread_mutex_t *thread_flow_mutex;
  pthread_cond_t *thread_switch;
  void ( *thread_unblock_proc )(P);
  pthread_t thread;
  P task_proc_arg;
      #endif
    #ifdef C_pthreads
  volatile B *thread_switch_in, *thread_switch_out;
    #endif
  I run_state_object;
  enum E_flow_Z_run_state run_state;
    #ifdef E_flow_C_thread_system_unblock_reports
  unsigned U_R( type, system_unblock_report )   :1;
    #endif
    #ifdef C_pthreads
  unsigned U_R( type, async )                   :1;
    #endif
};
    #if defined( E_flow_C_thread_system_unblock_reports ) || defined( C_pthreads )
struct E_flow_Q_task_async_Z_proc_args
{ P p;
  pthread_mutex_t *thread_flow_mutex;
  pthread_cond_t *thread_switch;
  volatile B *thread_switch_in, *thread_switch_out;
};
struct E_flow_Q_task_async_I_thread_proc_Z_args
{ stack_t alt_stack;
  void ( *task_proc )(P);
  struct E_flow_Q_task_async_Z_proc_args task_proc_arg;
};
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct E_flow_Q_report_Z
{ N uid;
  N reported_count;
};
struct E_flow_Q_timer_Z
{ Z_clock_time left;
  Z_clock_time period;
  N lost_count;
  N uid;
  I task_to;
};
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct E_flow_Q_process_call_srv_Z
{ P shm;
  pid_t process_id;
};
struct E_flow_Q_process_call_cli_Z
{ P shm;
  pid_t process_id;
  int shm_id;
  I task_id;
  unsigned U_R( state, active )         :1;
  unsigned U_R( state, ping )           :1;
  unsigned U_R( state, reply )          :1;
};
//==============================================================================
_internal
N
E_flow_Q_task_I_granulation( void
){  if( !E_base_S->E_flow_Q_task_S )
        return 0;
    N granulation_u = E_simple_T_multiply_overflow( E_base_S->E_flow_Z_task_stacks_S_n_pages, E_base_S->E_mem_S_page_size )
    || E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size > E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    ? E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    : E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size;
    granulation_u /= E_mem_Q_tab_R_n( E_base_S->E_flow_Q_task_S );
    granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u, E_base_S->E_mem_S_page_size );
    N r = 0;
    for_each_out( 0, task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
            #ifdef E_flow_C_thread_system_unblock_reports
        if( U_R( task->type, system_unblock_report )
        || U_R( task->type, async )
        )
        {
                #if defined( __gnu_linux__ )
            N ss_size = 2 * E_base_S->E_mem_S_page_size; //CONF
                #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
            N ss_size = E_simple_Z_n_I_align_up_to_v2( MINSIGSTKSZ, E_base_S->E_mem_S_page_size ); //CONF
                #else
#error not implemented
                #endif
            if( granulation_u < ss_size )
            {   r--;
                continue;
            }
            granulation_u -= ss_size;
            N pthread_stack_min = E_simple_Z_n_I_align_up_to_v2( PTHREAD_STACK_MIN, E_base_S->E_mem_S_page_size );
            if( granulation_u < pthread_stack_min )
            {   r--;
                continue;
            }
        }
            #endif
        if( granulation_u < E_base_S->E_mem_S_page_size )
        {   r--;
            continue;
        }
        if( task->stack_size > granulation_u )
            if( task->stack + task->stack_size - granulation_u <= task->touched_stack )
            {   N subtract = task->stack_size - granulation_u - E_base_S->E_mem_S_page_size;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }else
            {   N subtract = task->touched_stack - E_base_S->E_mem_S_page_size - task->stack;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
                r--;
            }
    }
    return r;
}
_export
I
E_flow_Q_task_M( I *uid
, void (*task_proc)(P)
    #ifdef E_flow_C_thread_system_unblock_reports
, P task_proc_arg
, B task_system_unblock_report
    #endif
    #ifdef C_line_report
, Pc task_proc_name
    #endif
){  N granulation_u = E_simple_T_multiply_overflow( E_base_S->E_flow_Z_task_stacks_S_n_pages, E_base_S->E_mem_S_page_size )
    || E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size > E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1
    ? E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    : E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size;
    granulation_u /= E_mem_Q_tab_R_n( E_base_S->E_flow_Q_task_S );
    granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u, E_base_S->E_mem_S_page_size );
        #ifdef E_flow_C_thread_system_unblock_reports
    struct E_flow_Q_task_async_I_thread_proc_Z_args *task_proc_args;
    if( task_system_unblock_report )
    {   M_( task_proc_args );
        if( !task_proc_args )
            return ~0;
        *task_proc_args = ( struct E_flow_Q_task_async_I_thread_proc_Z_args )
        { .alt_stack = ( stack_t )
          { .ss_flags = 0
              #if defined( __gnu_linux__ ) || defined( __FreeBSD__ ) || defined( __NetBSD__ )
          , .ss_size = 2 * E_base_S->E_mem_S_page_size //CONF
              #elif defined( __OpenBSD__ )
          , .ss_size = 3 * E_base_S->E_mem_S_page_size //CONF
              #else
#error not implemented
              #endif
          }
        , .task_proc = task_proc
        };
        if( granulation_u < task_proc_args->alt_stack.ss_size )
        {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
            return ~0;
        }
        granulation_u -= task_proc_args->alt_stack.ss_size;
    }
    N pthread_stack_min = E_simple_Z_n_I_align_up_to_v2( PTHREAD_STACK_MIN, E_base_S->E_mem_S_page_size );
    if( task_system_unblock_report )
    {   if( granulation_u < pthread_stack_min )
        {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
            return ~0;
        }
    }else if( granulation_u < E_base_S->E_mem_S_page_size )
    {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
        return ~0;
    }
        #else
    if( granulation_u < E_base_S->E_mem_S_page_size )
    {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
        return ~0;
    }
        #endif
    for_each_out( 0, task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
        if( task->stack_size > granulation_u )
            if( task->stack + task->stack_size - granulation_u <= task->touched_stack )
            {   N subtract = task->stack_size - granulation_u - E_base_S->E_mem_S_page_size;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }else
            {   GV_(NDFN); // Late instrumentation error: task stack larger than granulation unit.
                N subtract = task->touched_stack - E_base_S->E_mem_S_page_size - task->stack;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }
    }
    N stack_size;
    Pc stack;
    do
    {   stack_size = E_base_S->E_mem_S_page_size + granulation_u; // Jedna strona pamięci “PROT_NONE” na początku stosu.
        int errno_ = 0;
            #if defined( __gnu_linux__ )
        V1pe( stack = mmap( 0
          , stack_size
          , PROT_NONE
          , MAP_PRIVATE
          | MAP_ANONYMOUS | MAP_STACK | MAP_GROWSDOWN | MAP_UNINITIALIZED
          , -1
          , 0
          )
        , errno_
        )
            #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
        V1pe( stack = mmap( 0
          , stack_size
          , PROT_READ | PROT_WRITE
          , MAP_PRIVATE
          | MAP_ANON | MAP_STACK
          , -1
          , 0
          )
        , errno_
        )
            #else
    #error not implemented
            #endif
        {   if( errno_ != ENOMEM )
                return ~0;
            granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u / 2, E_base_S->E_mem_S_page_size );
                #ifndef E_flow_C_thread_system_unblock_reports
            if( granulation_u < E_base_S->E_mem_S_page_size )
                #else
            if( granulation_u < ( task_system_unblock_report ? pthread_stack_min : E_base_S->E_mem_S_page_size ))
                #endif
                return ~0;
        }
    }while( !~(N)stack );
    Pc exe_stack = stack + stack_size;
        #ifndef E_flow_C_thread_system_unblock_reports
    N touched_size = E_base_S->E_mem_S_page_size;
        #else
    N touched_size = task_system_unblock_report ? pthread_stack_min : E_base_S->E_mem_S_page_size;
        #endif
    Pc touched_stack = exe_stack - touched_size;
        #if defined( __gnu_linux__ )
    V0( mprotect( touched_stack
    , touched_size
    , PROT_READ | PROT_WRITE
    ))
        #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
    V0( mprotect( stack
    , stack_size - touched_size
    , PROT_NONE
    ))
        #else
#error not implemented
        #endif
    {   V0_( munmap( stack, stack_size ));
        return ~0;
    }
        #ifdef E_mem_Q_blk_C_debug
    E_mem_Q_blk_P_fill_c( touched_stack, touched_size, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
        #endif
    _Z_tasks_table_S_edit_begin_first;
    I task_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_task_S );
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
    task->stack = stack;
    task->touched_stack = touched_stack;
        #ifdef C_line_report
    task->proc_name = task_proc_name;
        #endif
    _Z_tasks_table_S_edit_end;
    task->run_state = E_flow_Z_run_state_S_ready;
    task->stack_size = stack_size;
        #ifdef C_pthreads
    U_L( task->type, async );
        #endif
        #ifdef E_flow_C_thread_system_unblock_reports
    U_R( task->type, system_unblock_report ) = task_system_unblock_report;
    if( task_system_unblock_report )
    {   volatile B *M_( thread_switch_in );
        if( !thread_switch_in )
        {   _Z_tasks_table_S_edit_begin;
            S r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        task->thread_switch_in = task_proc_args->task_proc_arg.thread_switch_in = thread_switch_in;
        volatile B *M_( thread_switch_out );
        if( !thread_switch_out )
        {   S r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        task->thread_switch_out = task_proc_args->task_proc_arg.thread_switch_out = thread_switch_out;
        pthread_mutex_t *thread_flow_mutex = E_mem_Q_blk_M_align( sizeof( *thread_flow_mutex ), sizeof(N32) );
        if( !thread_flow_mutex )
        {   S r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        task->thread_flow_mutex = task_proc_args->task_proc_arg.thread_flow_mutex = thread_flow_mutex;
        Vr( pthread_mutex_init( thread_flow_mutex, 0 ))
        {   S r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        pthread_cond_t *thread_switch = E_mem_Q_blk_M_align( sizeof( *thread_switch ), sizeof(N32) );
        if( !thread_switch )
        {   Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            S r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            if(r)
                return r;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        task->thread_switch = task_proc_args->task_proc_arg.thread_switch = thread_switch;
        Vr( pthread_cond_init( thread_switch, 0 ))
        {   S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        task->task_proc_arg = task_proc_args->task_proc_arg.p = task_proc_arg;
        *thread_switch_in = *thread_switch_out = no;
        task_proc_args->alt_stack.ss_size += E_base_S->E_mem_S_page_size;
            #if defined( __gnu_linux__ )
        V1p( task_proc_args->alt_stack.ss_sp = mmap( 0
        , task_proc_args->alt_stack.ss_size
        , PROT_READ | PROT_WRITE
        , MAP_PRIVATE
        | MAP_ANONYMOUS | MAP_STACK | MAP_GROWSDOWN | MAP_UNINITIALIZED
        , -1
        , 0
        ))
            #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
        V1p( task_proc_args->alt_stack.ss_sp = mmap( 0
        , task_proc_args->alt_stack.ss_size
        , PROT_READ | PROT_WRITE
        , MAP_PRIVATE
        | MAP_ANON | MAP_STACK
        , -1
        , 0
        ))
            #else
#error not implemented
            #endif
        {   Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        V0( mprotect( task_proc_args->alt_stack.ss_sp
        , E_base_S->E_mem_S_page_size
        , PROT_NONE
        ))
        {   V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
            #ifdef E_mem_Q_blk_C_debug
        E_mem_Q_blk_P_fill_c( task_proc_args->alt_stack.ss_sp + E_base_S->E_mem_S_page_size, task_proc_args->alt_stack.ss_size - E_base_S->E_mem_S_page_size, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
            #endif
        pthread_attr_t thread_attr;
        Vr( pthread_attr_init( &thread_attr ))
        {   V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        Vr( pthread_attr_setstack( &thread_attr, touched_stack, touched_size ))
        {   Vr_( pthread_attr_destroy( &thread_attr ));
            V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        Vr( pthread_attr_setguardsize( &thread_attr, 0 ))
        {   Vr_( pthread_attr_destroy( &thread_attr ));
            V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
            #if defined( __gnu_linux__ ) || defined( __FreeBSD__ )
        N cpu_n;
        V1( cpu_n = sysconf( _SC_NPROCESSORS_ONLN )){}
        if( ~cpu_n )
        {   cpu_set_t cpu_set;
            CPU_ZERO( &cpu_set );
            CPU_SET( 0, &cpu_set );
            Vr( pthread_attr_setaffinity_np( &thread_attr, sizeof( cpu_set ), &cpu_set ))
            {   Vr_( pthread_attr_destroy( &thread_attr ));
                V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
                Vr_( pthread_cond_destroy( thread_switch ));
                S r = W( thread_switch );
                if(r)
                    return r;
                Vr_( pthread_mutex_destroy( thread_flow_mutex ));
                r = W( thread_flow_mutex );
                if(r)
                    return r;
                r = W( thread_switch_out );
                if(r)
                    return r;
                r = W( thread_switch_in );
                if(r)
                    return r;
                _Z_tasks_table_S_edit_begin;
                r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
                _Z_tasks_table_S_edit_end;
                if(r)
                    return r;
                V0_( munmap( stack, stack_size ));
                return ~0;
            }
        }
            #endif
        Vr( pthread_sigmask( SIG_SETMASK, &E_base_S->E_flow_Z_sigset_S_empty, 0 ))
        {   Vr_( pthread_attr_destroy( &thread_attr ));
            V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        I task_id_ = E_base_S->E_flow_Q_task_S_current;
        E_base_S->E_flow_Q_task_S_current = task_id;
        Vr( pthread_create( &task->thread, &thread_attr, E_flow_Q_thread_system_unblock_report_I, task_proc_args ))
        {   E_base_S->E_flow_Q_task_S_current = task_id_;
            Vr_( pthread_attr_destroy( &thread_attr ));
            V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
            Vr_( pthread_cond_destroy( thread_switch ));
            S r = W( thread_switch );
            if(r)
                return r;
            Vr_( pthread_mutex_destroy( thread_flow_mutex ));
            r = W( thread_flow_mutex );
            if(r)
                return r;
            r = W( thread_switch_out );
            if(r)
                return r;
            r = W( thread_switch_in );
            if(r)
                return r;
            _Z_tasks_table_S_edit_begin;
            r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _Z_tasks_table_S_edit_end;
            if(r)
                return r;
            V0_( munmap( stack, stack_size ));
            return ~0;
        }
        Vr_( pthread_attr_destroy( &thread_attr ));
            #if defined( C_line_report ) && defined( __gnu_linux__ )
        Pc s = E_text_Z_s_R_search_c_( task->proc_name, 'E' ) + 2;
        Pc s_end = E_text_Z_s0_R_end_0_le( s, 16 );
        if( *s_end )
        {   s = E_text_Z_s_R_search_c_( task->proc_name, 'D' ) + 2;
            s_end = E_text_Z_s0_R_end_0_le( s, 16 );
        }
        B b = no;
        if( *s_end )
        {   Pc s_ = M( s_end + 1 - s + 1 );
            if( s_ )
            {   E_text_Z_s_P_copy_s_0( s_, s, s_end + 1 );
                s = s_;
            }else
            {   GV_(NA);
                b = yes;
            }
        }
        if( !b )
        {   Vr( pthread_setname_np( task->thread, s )){}
            if( *s_end )
                W(s);
        }
            #endif
        Vr_( pthread_mutex_lock( thread_flow_mutex ));
        if( !*thread_switch_in )
        {   Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
        }
        *thread_switch_in = no;
        Vr_( pthread_cond_signal( thread_switch ));
        E_base_S->E_flow_Q_task_S_current = task_id_;
        Vr_( pthread_mutex_unlock( thread_flow_mutex ));
        return task_id;
    }
        #endif
    task->exe_stack = 0;
    task->run_state_object = E_base_S->E_flow_Q_task_S_current;
        #ifdef E_flow_C_thread_system_unblock_reports
    task->thread = pthread_self();
        #endif
    P *p = ( P * )exe_stack - 1;
        #ifdef __i386__
    p--; // Przesunięcie o argument “task_proc” odłożony na stosie.
        #endif
    *p = (P)&E_flow_Q_task_I_stop;
    E_flow_Q_task_I_switch( task_id );
    if( !task->exe_stack ) // W bloku – nowe ‹zadanie›: nic nie zmieniać na stosie należącym do przełączanego.
    {   __asm__ volatile (
            #if defined( __i386__ ) || defined( __x86_64__ )
                #if defined( __i386__ )
        "\n" "mov   %0,%%esp"
                #else
        "\n" "mov   %0,%%rsp"
                #endif
        "\n" "jmp   *%1"
        :
        : "r" (p), "r" ( task_proc )
            #else
#error not implemented
            #endif
        );
        _unreachable;
    }
    *uid = task_id;
    return task_id;
}
    #ifdef E_flow_C_thread_system_unblock_reports
// Nie wolno tworzyć “wątku”/‘instancji’ tego samego ‹zadania› w bloku startowym (innej ‘instancji’) tego samego ‹zadania›.
_export
I
E_flow_Q_task_M_thread( I *uid
, I subid
, void (*task_proc)(P)
, P task_proc_arg
        #ifdef C_line_report
, Pc task_proc_name
        #endif
){  I uid_start = *uid;
    I id;
    if( (S)( id = E_flow_Q_task_M( uid, task_proc, task_proc_arg, yes
        #ifdef C_line_report
    , task_proc_name
        #endif
    )) < 0
    )
        return id;
    struct E_mem_Q_tab_Z **tab_subid;
    if( !~uid_start )
    {   *uid = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_task_S_uid_subid );
        tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
        *tab_subid = E_mem_Q_tab_M( sizeof(I), subid + 1 );
        if( !*tab_subid )
        {   S r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
            if(r)
                return r;
            r = E_flow_Q_task_W( &id );
            if(r)
                return r;
            return ~0;
        }
    }else
    {   tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
        S r = E_mem_Q_tab_I_add_i( E_base_S->E_flow_Q_task_S_uid_subid, subid );
        if(r)
            return r;
    }
    *( I * )E_mem_Q_tab_R( *tab_subid, subid ) = id;
    return *uid;
}
// Procedura ‹zadań› wątkowanych.
_internal
P
E_flow_Q_thread_system_unblock_report_I( P args_
){  Da_();
    struct E_flow_Q_task_async_I_thread_proc_Z_args *args = args_;
    V0( sigaltstack( &args->alt_stack, 0 ))
        return 0;
    Vr_( pthread_mutex_lock( args->task_proc_arg.thread_flow_mutex ));
    *args->task_proc_arg.thread_switch_in = yes;
    args->task_proc( &args->task_proc_arg );
    Vr_( pthread_mutex_unlock( args->task_proc_arg.thread_flow_mutex ));
    V0_( sigaltstack( 0, 0 ));
    V0_( munmap( args->alt_stack.ss_sp, args->alt_stack.ss_size ));
    return 0;
}
    #endif
_export
N
E_flow_Q_task_W( I *uid
){  I id = *uid;
    *uid = ~0;
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
    task->run_state = E_flow_Z_run_state_S_stopping_by_task;
    task->run_state_object = E_base_S->E_flow_Q_task_S_current;
        #ifdef E_flow_C_thread_system_unblock_reports
    if( U_R( task->type, system_unblock_report ))
    {   task->thread_unblock_proc( task->task_proc_arg );
        Vr_( pthread_mutex_lock( task->thread_flow_mutex ));
        while( !*task->thread_switch_out )
        {   Vr_( pthread_mutex_unlock( task->thread_flow_mutex ));
            V1_( sched_yield() );
            Vr_( pthread_mutex_lock( task->thread_flow_mutex ));
        }
        *task->thread_switch_in = yes;
        E_base_S->E_flow_Q_task_S_current = id;
        Vr_( pthread_mutex_unlock( task->thread_flow_mutex ));
        Vr_( pthread_join( task->thread, 0 ));
        E_base_S->E_flow_Q_task_S_current = task->run_state_object;
        Vr_( pthread_mutex_destroy( task->thread_flow_mutex ));
        N r = W( task->thread_flow_mutex );
        if(r)
            return r;
        Vr_( pthread_cond_destroy( task->thread_switch ));
        r = W( task->thread_switch );
        if(r)
            return r;
        r = W( task->thread_switch_out );
        if(r)
            return r;
        r = W( task->thread_switch_in );
        if(r)
            return r;
        if( task->task_proc_arg )
        {   r = W( task->task_proc_arg );
            if(r)
                return r;
        }
    }else
        #endif
    {   E_flow_Q_task_I_switch(id); // Przełącz tylko po to, by ‹zadanie› zwalniane zwolniło zasoby; również stosowo, hierarchicznie z powrotem przełączając przy zwalnianiu ‹zadań› przez siebie uruchomionych.
        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
    }
    _Z_tasks_table_S_edit_begin_first;
    V0_( munmap( task->stack, task->stack_size ));
    N r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, id );
    _Z_tasks_table_S_edit_end;
    if(r)
        return r;
    return 0;
}
    #ifdef E_flow_C_thread_system_unblock_reports
_export
N
E_flow_Q_task_W_thread( I *uid
, I subid
){  struct E_mem_Q_tab_Z **tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
    I id = *( I * )E_mem_Q_tab_R( *tab_subid, subid );
    if( E_mem_Q_tab_R_n( *tab_subid ) != 1 )
    {   N r = E_mem_Q_tab_I_remove( *tab_subid, subid );
        if(r)
            return r;
    }else
    {   N r = E_mem_Q_tab_W( *tab_subid );
        if(r)
            return r;
        *uid = ~0;
    }
    return E_flow_Q_task_W( &id );
}
    #endif
// ‹Zadanie› synchroniczne wykonuje to po wyjściu z procedury; ma adres powrotu na stosie.
_internal
void
E_flow_Q_task_I_stop( void
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    E_flow_Q_task_I_switch( task->run_state_object ); // Powrót do ‹zadania› zwalniającego bieżące.
    _unreachable;
}
//------------------------------------------------------------------------------
// Wymuszenie ‘prealokacji’ “stosu” ‹zadania›, by nie została wywołana procedura obsługi “sygnału” ‘uniksowego’ “SEGV” w trakcie zmieniania pamięci ‘alokowanej’ dynamicznie, z której korzysta.
_internal
void
E_flow_Q_task_I_touch_stack( N page_count
){  if( !E_base_S->E_flow_Q_task_S_current ) // ‹Zadanie› inne niż “main”, ponieważ to ma stos zarządzany przez system operacyjny.
        return;
    N pages = page_count ? page_count : 4; //CONF
    volatile Pc sp;
    __asm__ volatile (
    #if defined( __i386__ ) || defined( __x86_64__ )
        #if defined( __i386__ )
    "\n" "mov   %%esp,%0"
        #else
    "\n" "mov   %%rsp,%0"
        #endif
    #else
#error not implemented
    #endif
    : "=r" (sp)
    );
    // Dlatego kolejno “strony” pamięci, ponieważ jest tylko jedna zabezpieczająca na dole “stosu” ‹zadania›.
    while( pages-- )
    {   sp -= E_base_S->E_mem_S_page_size;
        *sp = 0;
    }
}
//------------------------------------------------------------------------------
    #ifdef C_pthreads
_export
I
E_flow_Q_task_async_M( I *uid
, void (*task_proc)(P)
,  void ( *thread_unblock_proc )(P)
        #ifdef C_line_report
, Pc task_proc_name
        #endif
){  N granulation_u = E_simple_T_multiply_overflow( E_base_S->E_flow_Z_task_stacks_S_n_pages, E_base_S->E_mem_S_page_size )
    || E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size > E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1
    ? E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    : E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size;
    granulation_u /= E_mem_Q_tab_R_n( E_base_S->E_flow_Q_task_S );
    granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u, E_base_S->E_mem_S_page_size );
    struct E_flow_Q_task_async_I_thread_proc_Z_args *M_( task_proc_args );
    if( !task_proc_args )
        return ~0;
    *task_proc_args = ( struct E_flow_Q_task_async_I_thread_proc_Z_args )
    { .alt_stack = ( stack_t )
      { .ss_flags = 0
          #if defined( __gnu_linux__ ) || defined( __FreeBSD__ ) || defined( __NetBSD__ )
      , .ss_size = 2 * E_base_S->E_mem_S_page_size //CONF
          #elif defined( __OpenBSD__ )
      , .ss_size = 3 * E_base_S->E_mem_S_page_size //CONF
          #else
#error not implemented
          #endif
      }
    , .task_proc = task_proc
    };
    if( granulation_u < task_proc_args->alt_stack.ss_size )
    {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
        return ~0;
    }
    granulation_u -= task_proc_args->alt_stack.ss_size;
    N pthread_stack_min = E_simple_Z_n_I_align_up_to_v2( PTHREAD_STACK_MIN, E_base_S->E_mem_S_page_size );
    if( granulation_u < pthread_stack_min )
    {   GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
        return ~0;
    }
    for_each_out( 0, task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
        if( task->stack_size > granulation_u )
            if( task->stack + task->stack_size - granulation_u <= task->touched_stack )
            {   N subtract = task->stack_size - granulation_u - E_base_S->E_mem_S_page_size;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }else
            {   GV_(NDFN); // Late instrumentation error: task stack larger than granulation unit.
                N subtract = task->touched_stack - E_base_S->E_mem_S_page_size - task->stack;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }
    }
    N stack_size;
    Pc stack;
    do
    {   stack_size = E_base_S->E_mem_S_page_size + granulation_u; // Jedna strona pamięci “PROT_NONE” na początku stosu.
        int errno_ = 0;
            #if defined( __gnu_linux__ )
        V1pe( stack = mmap( 0
          , stack_size
          , PROT_NONE
          , MAP_PRIVATE
          | MAP_ANONYMOUS | MAP_STACK | MAP_GROWSDOWN | MAP_UNINITIALIZED
          , -1
          , 0
          )
        , errno_
        )
            #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
        V1pe( stack = mmap( 0
          , stack_size
          , PROT_READ | PROT_WRITE
          , MAP_PRIVATE
          | MAP_ANON | MAP_STACK
          , -1
          , 0
          )
        , errno_
        )
            #else
    #error not implemented
            #endif
        {   if( errno_ != ENOMEM )
                return ~0;
            granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u / 2, E_base_S->E_mem_S_page_size );
            if( granulation_u < pthread_stack_min )
                return ~0;
        }
    }while( !~(N)stack );
    Pc exe_stack = stack + stack_size;
    Pc touched_stack = exe_stack - pthread_stack_min;
        #if defined( __gnu_linux__ )
    V0( mprotect( touched_stack
    , pthread_stack_min
    , PROT_READ | PROT_WRITE
    ))
        #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
    V0( mprotect( stack
    , stack_size - pthread_stack_min
    , PROT_NONE
    ))
        #else
#error not implemented
        #endif
    {   V0_( munmap( stack, stack_size ));
        return ~0;
    }
        #ifdef E_mem_Q_blk_C_debug
    E_mem_Q_blk_P_fill_c( touched_stack, pthread_stack_min, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
        #endif
    _Z_tasks_table_S_edit_begin_first;
    I task_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_task_S );
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
    task->stack = stack;
    task->touched_stack = touched_stack;
        #ifdef C_line_report
    task->proc_name = task_proc_name;
        #endif
    _Z_tasks_table_S_edit_end;
    task->run_state = E_flow_Z_run_state_S_ready;
    task->stack_size = stack_size;
    U_L( task->type, system_unblock_report );
    U_F( task->type, async );
    task->thread_unblock_proc = thread_unblock_proc;
    volatile B *M_( thread_switch_in );
    if( !thread_switch_in )
    {   _Z_tasks_table_S_edit_begin;
        S r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    task->thread_switch_in = task_proc_args->task_proc_arg.thread_switch_in = thread_switch_in;
    volatile B *M_( thread_switch_out );
    if( !thread_switch_out )
    {   S r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    task->thread_switch_out = task_proc_args->task_proc_arg.thread_switch_out = thread_switch_out;
    pthread_mutex_t *thread_flow_mutex = E_mem_Q_blk_M_align( sizeof( *thread_flow_mutex ), sizeof(N32) );
    if( !thread_flow_mutex )
    {   S r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    task->thread_flow_mutex = task_proc_args->task_proc_arg.thread_flow_mutex = thread_flow_mutex;
    Vr( pthread_mutex_init( thread_flow_mutex, 0 ))
    {   S r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    pthread_cond_t *M_( thread_switch );
    if( !thread_switch )
    {   Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        S r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    task->thread_switch = task_proc_args->task_proc_arg.thread_switch = thread_switch;
    Vr( pthread_cond_init( thread_switch, 0 ))
    {   S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    *thread_switch_in = *thread_switch_out = no;
    task_proc_args->alt_stack.ss_size += E_base_S->E_mem_S_page_size;
        #if defined( __gnu_linux__ )
    V1p( task_proc_args->alt_stack.ss_sp = mmap( 0
    , task_proc_args->alt_stack.ss_size
    , PROT_READ | PROT_WRITE
    , MAP_PRIVATE
    | MAP_ANONYMOUS | MAP_STACK | MAP_GROWSDOWN | MAP_UNINITIALIZED
    , -1
    , 0
    ))
        #elif defined( __FreeBSD__ ) || defined( __NetBSD__ ) || defined( __OpenBSD__ )
    V1p( task_proc_args->alt_stack.ss_sp = mmap( 0
    , task_proc_args->alt_stack.ss_size
    , PROT_READ | PROT_WRITE
    , MAP_PRIVATE
    | MAP_ANON | MAP_STACK
    , -1
    , 0
    ))
        #else
#error not implemented
        #endif
    {   Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    V0( mprotect( task_proc_args->alt_stack.ss_sp
    , E_base_S->E_mem_S_page_size
    , PROT_NONE
    ))
    {   V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
        #ifdef E_mem_Q_blk_C_debug
    E_mem_Q_blk_P_fill_c( task_proc_args->alt_stack.ss_sp + E_base_S->E_mem_S_page_size, task_proc_args->alt_stack.ss_size - E_base_S->E_mem_S_page_size, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
        #endif
    pthread_attr_t thread_attr;
    Vr( pthread_attr_init( &thread_attr ))
    {   V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    Vr( pthread_attr_setstack( &thread_attr, touched_stack, pthread_stack_min ))
    {   Vr_( pthread_attr_destroy( &thread_attr ));
        V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    Vr( pthread_attr_setguardsize( &thread_attr, 0 ))
    {   Vr_( pthread_attr_destroy( &thread_attr ));
        V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
        #if defined( __gnu_linux__ ) || defined( __FreeBSD__ )
    if( E_base_S->cpu_taken )
    {   N cpu_n;
        V1( cpu_n = sysconf( _SC_NPROCESSORS_ONLN )){}
        if( ~cpu_n
        && cpu_n > 1
        )
        {   cpu_n--;
            B fail = no;
            if( cpu_n != E_base_S->cpu_taken_length )
                if( ~E_mem_Q_mask_I_resize( &E_base_S->cpu_taken, E_base_S->cpu_taken_length, cpu_n ))
                    E_base_S->cpu_taken_length = cpu_n;
                else
                    fail = yes;
            if( !fail )
            {   cpu_set_t cpu_set;
                CPU_ZERO( &cpu_set );
                N cpu_i = E_mem_Q_mask_R_first_clear( E_base_S->cpu_taken, E_base_S->cpu_taken_length );
                if( ~cpu_i )
                {   CPU_SET( cpu_i + 1, &cpu_set );
                    E_mem_Q_mask_P_set( E_base_S->cpu_taken, cpu_i );
                    Vr( pthread_attr_setaffinity_np( &thread_attr, sizeof( cpu_set ), &cpu_set ))
                    {   Vr_( pthread_attr_destroy( &thread_attr ));
                        V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
                        Vr_( pthread_cond_destroy( thread_switch ));
                        S r = W( thread_switch );
                        if(r)
                            return r;
                        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
                        r = W( thread_flow_mutex );
                        if(r)
                            return r;
                        r = W( thread_switch_out );
                        if(r)
                            return r;
                        r = W( thread_switch_in );
                        if(r)
                            return r;
                        _Z_tasks_table_S_edit_begin;
                        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
                        _Z_tasks_table_S_edit_end;
                        if(r)
                            return r;
                        V0_( munmap( stack, stack_size ));
                        return ~0;
                    }
                }
            }
        }
    }
        #endif
    Vr( pthread_sigmask( SIG_SETMASK, &E_base_S->E_flow_Z_sigset_S_empty, 0 ))
    {   Vr_( pthread_attr_destroy( &thread_attr ));
        V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    Vr( pthread_create( &task->thread, &thread_attr, E_flow_Q_thread_async_I, task_proc_args ))
    {   Vr_( pthread_attr_destroy( &thread_attr ));
        V0_( munmap( task_proc_args->alt_stack.ss_sp, task_proc_args->alt_stack.ss_size ));
        Vr_( pthread_cond_destroy( thread_switch ));
        S r = W( thread_switch );
        if(r)
            return r;
        Vr_( pthread_mutex_destroy( thread_flow_mutex ));
        r = W( thread_flow_mutex );
        if(r)
            return r;
        r = W( thread_switch_out );
        if(r)
            return r;
        r = W( thread_switch_in );
        if(r)
            return r;
        _Z_tasks_table_S_edit_begin;
        r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
        _Z_tasks_table_S_edit_end;
        if(r)
            return r;
        V0_( munmap( stack, stack_size ));
        return ~0;
    }
    Vr_( pthread_attr_destroy( &thread_attr ));
        #if defined( C_line_report ) && defined( __gnu_linux__ )
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
    Pc s = E_text_Z_s_R_search_c_( task->proc_name, 'E' ) + 2;
    Pc s_end = E_text_Z_s0_R_end_0_le( s, 16 );
    if( *s_end )
    {   s = E_text_Z_s_R_search_c_( task->proc_name, 'D' ) + 2;
        s_end = E_text_Z_s0_R_end_0_le( s, 16 );
    }
    B b = no;
    if( *s_end )
    {   Pc s_ = M( s_end + 1 - s + 1 );
        if( s_ )
        {   E_text_Z_s_P_copy_s_0( s_, s, s_end + 1 );
            s = s_;
        }else
        {   GV_(NA);
            b = yes;
        }
    }
    if( !b )
    {   Vr( pthread_setname_np( task->thread, s )){}
        if( *s_end )
            W(s);
    }
        #endif
    *uid = task_id;
    return task_id;
}
// Procedura ‹zadań› asynchronicznych.
_internal
P
E_flow_Q_thread_async_I( P args_
){  Da_();
    struct E_flow_Q_task_async_I_thread_proc_Z_args *args = args_;
    V0( sigaltstack( &args->alt_stack, 0 ))
        return 0;
    Vr_( pthread_mutex_lock( args->task_proc_arg.thread_flow_mutex ));
    args->task_proc( &args->task_proc_arg );
    Vr_( pthread_mutex_unlock( args->task_proc_arg.thread_flow_mutex ));
    V0_( sigaltstack( 0, 0 ));
    V0_( munmap( args->alt_stack.ss_sp, args->alt_stack.ss_size ));
    return 0;
}
_export
N
E_flow_Q_task_async_W( I *uid
){  I id = *uid;
    *uid = ~0;
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
    task->run_state = E_flow_Z_run_state_S_stopping_by_task;
    task->run_state_object = E_base_S->E_flow_Q_task_S_current;
        #if defined( __gnu_linux__ ) || defined( __FreeBSD__ )
    int ret_;
    cpu_set_t cpu_set;
    Vr( ret_ = pthread_getaffinity_np( task->thread, sizeof( cpu_set ), &cpu_set )){}
    if( !ret_ )
    {   N cpu_i;
        for( cpu_i = 1; cpu_i != J_min( CPU_SETSIZE, E_base_S->cpu_taken_length + 1 ); cpu_i++ )
            if( CPU_ISSET( cpu_i, &cpu_set ))
                break;
        if( cpu_i != J_min( CPU_SETSIZE, E_base_S->cpu_taken_length + 1 ))
            E_mem_Q_mask_P_clear( E_base_S->cpu_taken, cpu_i - 1 );
    }
        #endif
    task->thread_unblock_proc( task->task_proc_arg );
    Vr_( pthread_mutex_lock( task->thread_flow_mutex ));
    while( !*task->thread_switch_out ) // Oczekuje aż się zatrzyma na “Da_B_” lub wyjdzie z procedury wątku.
    {   Vr_( pthread_mutex_unlock( task->thread_flow_mutex ));
        V1_( sched_yield() );
        Vr_( pthread_mutex_lock( task->thread_flow_mutex ));
    }
    *task->thread_switch_in = yes;
    E_base_S->E_flow_Q_task_S_current = id;
    Vr_( pthread_mutex_unlock( task->thread_flow_mutex ));
    Vr_( pthread_join( task->thread, 0 ));
    E_base_S->E_flow_Q_task_S_current = task->run_state_object;
    Vr_( pthread_mutex_destroy( task->thread_flow_mutex ));
    N r = W( task->thread_flow_mutex );
    if(r)
        return r;
    Vr_( pthread_cond_destroy( task->thread_switch ));
    r = W( task->thread_switch );
    if(r)
        return r;
    r = W( task->thread_switch_out );
    if(r)
        return r;
    r = W( task->thread_switch_in );
    if(r)
        return r;
    _Z_tasks_table_S_edit_begin_first;
    V0_( munmap( task->stack, task->stack_size ));
    r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, id );
    _Z_tasks_table_S_edit_end;
    return r;
}
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_export
I
E_flow_Q_report_M( N uid
){  struct E_flow_Q_report_Z *report;
    for_each( report_id, E_base_S->E_flow_Q_report_S, E_mem_Q_tab )
    {   report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, report_id );
        if( report->uid == uid )
            break;
    }
    if( !~report_id )
    {   report_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_report_S );
        if( (S)report_id < 0 )
            return (S)report_id;
        report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, report_id );
        report->uid = uid;
        report->reported_count = 0;
    }
    return report_id;
}
_export
N
E_flow_Q_report_W( I id
){  return E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_report_S, id );
}
//------------------------------------------------------------------------------
_export
void
E_flow_Q_report_I_signal( I id
){  struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    if( ~report->reported_count )
        report->reported_count++;
    for_each( task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
            #ifdef C_pthreads
        if( U_R( task->type, async ))
            continue;
            #endif
        if( task->run_state == E_flow_Z_run_state_S_waiting_for_report
        && task->run_state_object == id
        )
        {   task->run_state = E_flow_Z_run_state_S_ready;
            break;
        }
    }
}
_export
B
E_flow_Q_report_I_wait( I id
, N *lost_count
){  B r;
    struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    if( report->reported_count )
    {   //for_each( task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
        //{   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
                //#ifdef C_pthreads
            //if( U_R( task->type, async ))
                //continue;
                //#endif
            //if( task->run_state == E_flow_Z_run_state_S_emiting_report
            //&& task->run_state_object == id
            //)
                //task->run_state = E_flow_Z_run_state_S_ready;
        //}
        r = no; // Nie wywołuje “schedule”, ponieważ w przełączanym tylko w oznaczonych punktach przepływie wykonania — bieżące ‹zadanie› mogło umożliwić zaistnienie ‹raportu›, na który czeka, tylko wtedy, jeśli przełącza do innych ‹zadań› ·w innych punktach niż to oczekiwanie na ‹raport›·, więc po co czekać, skoro nie zaburza cyklu przełączania ‹zadań›, a tylko w implementacji własnego ‹zadania› zmienia na złożoną (przesuniętą) sekwencję przełączania.
    }else
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
        task->run_state = E_flow_Z_run_state_S_waiting_for_report;
        task->run_state_object = id;
        r = E_flow_Q_task_I_schedule();
        report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    }
    if( lost_count )
        *lost_count = report->reported_count - 1;
    report->reported_count = 0;
    return r;
}
_export
void
E_flow_Q_report_I_clear( I id
){  struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    report->reported_count = 0;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_export
I
E_flow_Q_timer_M( N period
){  Z_clock_time tv;
    _gettime( &tv );
    I timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
    if( timer_id < 0 )
        return timer_id;
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
    timer->period.tv_sec = period / 1000000;
    timer->period.Z_clock_time_minor_field = period % 1000000;
        #ifdef E_flow_drv_C_clock_monotonic
    timer->period.Z_clock_time_minor_field *= 1000;
        #endif
    timer->lost_count = 0;
    timer->uid = ~0;
    timer->task_to = E_base_S->E_flow_Q_task_S_current;
    if( E_mem_Q_tab_R_n( E_base_S->E_flow_Q_timer_S ) != 1 )
    {   for_each_out( timer_id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
        {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
            if( _timerisset( &timer_->period ) // Jest co najmniej jeden ‹cykler›
            || _timerisset( &timer_->left ) // lub co najmniej jeden wzbudzony ‹impulsator›.
            )
            {   _timeradd( &tv, &timer->period, &tv );
                if( _timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ))
                    E_base_S->E_flow_Q_timer_S_next_real_time = tv;
                _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &timer->left );
                return timer_id;
            }
        }
    }
    E_base_S->E_flow_Q_timer_S_last_real_time = tv;
    _timeradd( &tv, &timer->period, &E_base_S->E_flow_Q_timer_S_next_real_time );
    timer->left = timer->period;
    return timer_id;
}
_export
N
E_flow_Q_timer_W( I id
){  N r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_timer_S, id );
    if(r)
        return r;
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( _timerisset( &timer->period )
        || _timerisset( &timer->left )
        )
            return 0;
    }
    _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
    return 0;
}
//------------------------------------------------------------------------------
_export
B
E_flow_Q_timer_I_wait( I id
, N *lost_count
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_timer;
    task->run_state_object = id;
    B r = E_flow_Q_task_I_schedule();
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    if( lost_count )
        *lost_count = timer->lost_count;
    timer->lost_count = 0;
    return r;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_export
I
E_flow_Q_impulser_M( N uid
){  struct E_flow_Q_timer_Z *timer;
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( timer->uid == uid )
            break;
    }
    if( !~timer_id )
    {   timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
        if( (S)timer_id < 0 )
            return (S)timer_id;
        timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        _timerclear( &timer->left );
        _timerclear( &timer->period );
        timer->uid = uid;
    }
    timer->task_to = E_base_S->E_flow_Q_task_S_current;
    return timer_id;
}
_export
I
E_flow_Q_impulser_M_srv( N uid
){  struct E_flow_Q_timer_Z *timer;
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( timer->uid == uid )
            break;
    }
    if( !~timer_id )
    {   timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
        if( (S)timer_id < 0 )
            return (S)timer_id;
        timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        _timerclear( &timer->left );
        _timerclear( &timer->period );
        timer->uid = uid;
    }
    return timer_id;
}
//------------------------------------------------------------------------------
_export
void
E_flow_Q_impulser_I_activate( I id
, N time
){  Z_clock_time tv;
    _gettime( &tv );
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    timer->left.tv_sec = time / 1000000;
    timer->left.Z_clock_time_minor_field = time % 1000000;
        #ifdef E_flow_drv_C_clock_monotonic
    timer->left.Z_clock_time_minor_field *= 1000;
        #endif
    if( E_mem_Q_tab_R_n( E_base_S->E_flow_Q_timer_S ) != 1 )
    {   for_each_out( id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
        {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
            if( _timerisset( &timer_->period )
            || _timerisset( &timer_->left )
            )
            {   _timeradd( &tv, &timer->left, &tv );
                if( _timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ))
                    E_base_S->E_flow_Q_timer_S_next_real_time = tv;
                _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &timer->left );
                return;
            }
        }
    }
    E_base_S->E_flow_Q_timer_S_last_real_time = tv;
    _timeradd( &tv, &timer->left, &E_base_S->E_flow_Q_timer_S_next_real_time );
}
_export
void
E_flow_Q_impulser_I_deactivate( I id
){  struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    if( !_timerisset( &timer->left ))
        return;
    _timerclear( &timer->left );
    for_each_out( id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
        if( _timerisset( &timer_->period )
        || _timerisset( &timer_->left )
        )
            return;
    }
    _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
}
//------------------------------------------------------------------------------
//NDFN Dodać “lost_count”?
_export
B
E_flow_Q_impulser_I_wait( I id
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_timer;
    task->run_state_object = id;
    return E_flow_Q_task_I_schedule();
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    #ifdef E_flow_C_thread_system_unblock_reports
_export
void
E_flow_Q_thread_system_unblock_report_M( void ( *thread_unblock_proc )(P)
, pthread_mutex_t **thread_flow_mutex
, pthread_cond_t **thread_switch
, volatile B **thread_switch_in
, volatile B **thread_switch_out
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->thread_unblock_proc = thread_unblock_proc;
    *thread_flow_mutex = task->thread_flow_mutex;
    *thread_switch = task->thread_switch;
    *thread_switch_in = task->thread_switch_in;
    *thread_switch_out = task->thread_switch_out;
}
_export
void
E_flow_Q_thread_system_unblock_report_I_before_async(
  pthread_cond_t *thread_switch
, pthread_mutex_t *thread_flow_mutex
){  Vr_( pthread_cond_signal( thread_switch ));
    Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
}
_export
B
E_flow_Q_thread_system_unblock_report_I_after_async(
  volatile B *thread_switch_in
, volatile B *thread_switch_out
, pthread_cond_t *thread_switch
, pthread_mutex_t *thread_flow_mutex
){  Vr_( pthread_mutex_lock( &E_base_S->E_flow_Z_task_table_S_mutex ));
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, 0 );
    pthread_t thread = task->thread;
    Vr_( pthread_mutex_unlock( &E_base_S->E_flow_Z_task_table_S_mutex ));
    *thread_switch_out = yes;
    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
    while( !*thread_switch_in )
    {   if( !U_R( E_base_S->E_flow_S_signal, wake ))
        {   Vr_( pthread_kill( thread, SIGALRM ));
        }
        V1_( sched_yield() );
    }
    Vr_( pthread_mutex_lock( thread_flow_mutex ));
    *thread_switch_in = no;
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
}
_export
void
E_flow_Q_thread_system_unblock_report_I_unblock( I task_uid
, I task_subid
){  struct E_mem_Q_tab_Z **tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, task_uid );
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, *( I * )E_mem_Q_tab_R( *tab_subid, task_subid ));
    task->thread_unblock_proc( task->task_proc_arg );
}
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    #ifdef E_flow_C_itimer_system_unblock_report
_export
void
E_flow_Q_itimer_system_unblock_report_M( B (*sigsuspend_)( sigset_t * )
, void (*setitimer_)( Z_clock_time * )
){  E_base_S->E_flow_Z_itimer_system_unblock_report_I_sigsuspend = sigsuspend_;
    E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer = setitimer_;
    E_base_S->E_flow_Z_itimer_system_unblock_report_S_task_id = E_base_S->E_flow_Q_task_S_current;
}
_export
void
E_flow_Q_itimer_system_unblock_report_W( void
){  E_base_S->E_flow_Z_itimer_system_unblock_report_I_sigsuspend = 0;
    E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer = 0;
}
//------------------------------------------------------------------------------
_export
B
E_flow_Q_itimer_system_unblock_report_I_wait( void
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_system_unblock_report;
    return E_flow_Q_task_I_schedule();
}
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    #ifdef C_pthreads
_export
B
E_flow_Q_thread_async_I_before_sync(
  volatile B *thread_switch_in
, volatile B *thread_switch_out
, pthread_cond_t *thread_switch
, pthread_mutex_t *thread_flow_mutex
){  Vr_( pthread_mutex_lock( &E_base_S->E_flow_Z_task_table_S_mutex ));
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, 0 );
    pthread_t thread = task->thread;
    Vr_( pthread_mutex_unlock( &E_base_S->E_flow_Z_task_table_S_mutex ));
    *thread_switch_out = yes;
    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
    while( !*thread_switch_in )
    {   if( !U_R( E_base_S->E_flow_S_signal, wake ))
        {   Vr_( pthread_kill( thread, SIGALRM ));
        }
        V1_( sched_yield() );
    }
    Vr_( pthread_mutex_lock( thread_flow_mutex ));
    *thread_switch_in = no;
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
}
_export
void
E_flow_Q_thread_async_I_after_sync(
  pthread_cond_t *thread_switch
, pthread_mutex_t *thread_flow_mutex
){  Vr_( pthread_cond_signal( thread_switch ));
    Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
}
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Każde ‹zadanie› synchroniczne po wywołaniu “E_flow_Q_task_I_schedule” i po przełączeniu w tej procedurze do innego ‹zadania› (“E_flow_Q_task_I_switch”) czeka przed instrukcją powrotu z tej procedury, by kontynuować w miejscu wywołania i ewentualnie zakończyć własne ‹zadanie› po powrocie.
_export
__attribute__ (( __noinline__, __returns_twice__, __hot__ ))
B
E_flow_Q_task_I_schedule( void
){  _forced_statement;
        #ifdef E_flow_C_thread_system_unblock_reports
    I schedule_task_id = E_base_S->E_flow_Q_task_S_current;
        #endif
        #if defined( E_flow_C_thread_system_unblock_reports ) || defined( C_pthreads )
Loop:
        #endif
    O{  if( U_E( E_base_S->E_flow_S_signal, exit ))
        {
                #ifndef C_middle_code
            U_F( E_base_S->E_flow_S_signal, exit_all );
                #endif
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, 0 );
            task->run_state = E_flow_Z_run_state_S_stopping_by_task;
                #ifdef E_flow_C_thread_system_unblock_reports
            if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                E_base_S->E_flow_Q_task_S_current = schedule_task_id;
            if( E_base_S->E_flow_Q_task_S_current )
                #endif
            {   E_flow_Q_task_I_switch(0);
                task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
                return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
            }
                #ifdef E_flow_C_thread_system_unblock_reports
            return yes;
                #endif
        }
        if( U_E( E_base_S->E_flow_S_signal, call_req ))
            X_F( flow, call_req );
        if( U_E( E_base_S->E_flow_S_signal, io_ready ))
            X_F( io, stream_write );
        Z_clock_time next_real_time;
        _timerover( &next_real_time );
        Z_clock_time tv;
        _gettime( &tv );
        _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
        for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
        {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
            if( !U_R( call->state, active ))
                continue;
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, call->task_id );
            if( _timercmp( &tv, <, &task->run_state_time ))
            {   if( _timercmp( &task->run_state_time, <, &next_real_time ))
                    next_real_time = task->run_state_time;
            }else if( U_R( call->state, ping ))
            {   task->run_state = E_flow_Z_run_state_S_ready;
                U_L( call->state, active );
            }else
            {   U_F( call->state, ping );
                if( kill( call->process_id, SIGVTALRM ))
                {   task->run_state = E_flow_Z_run_state_S_ready;
                    U_L( call->state, active );
                }else
                {   _timeradd( &tv, &E_base_S->E_flow_Q_process_call_S_ping_period, &task->run_state_time );
                    if( _timercmp( &task->run_state_time, <, &next_real_time ))
                        next_real_time = task->run_state_time;
                }
            }
        }
        _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
        if( U_E( E_base_S->E_flow_S_signal, time ) // Na pewno poniższa linia, bo przychodzi z obudzenia po upływie tego czasu.
        || !_timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ) // Jeśli nie przychodzi z obudzenia, a z wywołania w którymś ‹zadaniu›, to trzeba sprawdzić najbliższy czas ‹cyklerów>.
        ) // Czy trzeba uaktualnić kolejne czasy ‹cyklerów›.
        {   Z_clock_time elapsed_time;
            _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &elapsed_time );
            //NDFN Uzupełnić o jakieś przewidywanie ‘overhead’ na podstawie poprzedniego, by wyeliminować możliwość powtarzania pętli w pesymistycznym przypadku dla każdego ‹cyklera›? Ale obliczać ten czas tylko wtedy, jeżeli ten fragment nie będzie mógł być wywłaszczony z wykonywania w czasie rzeczywistym (wszystkie przerwania wyłączone).
            O{  B some_timer_is_active = no, some_timer_has_deactivated = no;
                B some_task_got_ready = no;
                Z_clock_time suspend_time;
                _timerover( &suspend_time );
                for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
                {   struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
                    if( _timerisset( &timer->period )) // ‹cykler›.
                    {   if( !_timercmp( &elapsed_time, <, &timer->left )) // ‹cykler› wykonał obieg— ‹zadanie› do wznowienia.
                        {   Z_clock_time overlate_time;
                            _timersub( &elapsed_time, &timer->left, &overlate_time );
                            if( !_timercmp( &overlate_time, <, &timer->period )) // ‹cykler› wykonał więcej niż jeden obieg.
                            {   do
                                {   timer->lost_count++;
                                    _timersub( &overlate_time, &timer->period, &overlate_time );
                                }while( !_timercmp( &overlate_time, <, &timer->period ));
                                GV_(NA); Gd( timer->lost_count ); Gd( timer->period.tv_sec ); Gd( timer->period.Z_clock_time_minor_field ); Gd( overlate_time.tv_sec ); Gd( overlate_time.Z_clock_time_minor_field ); // lost time.
                            }
                            _timersub( &timer->period, &overlate_time, &timer->left );
                            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, timer->task_to );
                            if( task->run_state == E_flow_Z_run_state_S_waiting_for_timer
                            && task->run_state_object == timer_id
                            )
                            {   task->run_state = E_flow_Z_run_state_S_ready;
                                some_task_got_ready = yes;
                            }else
                                timer->lost_count++;
                        }else
                            _timersub( &timer->left, &elapsed_time, &timer->left );
                        if( _timercmp( &timer->left, <, &suspend_time ))
                        {   suspend_time = timer->left;
                            some_timer_is_active = yes;
                        }
                    }else if( _timerisset( &timer->left )) // Aktywowany ‹impulsator›.
                    {   if( !_timercmp( &elapsed_time, <, &timer->left ))
                        {   _timerclear( &timer->left );
                            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, timer->task_to );
                            if( task->run_state == E_flow_Z_run_state_S_waiting_for_timer
                            && task->run_state_object == timer_id
                            )
                            {   task->run_state = E_flow_Z_run_state_S_ready;
                                some_task_got_ready = yes;
                            } // Narazie – jeśli nie może wznowić ‹zadania›, to gubi impuls.
                            some_timer_has_deactivated = yes;
                        }else
                        {   _timersub( &timer->left, &elapsed_time, &timer->left );
                            if( _timercmp( &timer->left, <, &suspend_time ))
                            {   suspend_time = timer->left;
                                some_timer_is_active = yes;
                            }
                        }
                    }
                }
                if( some_timer_has_deactivated
                && !some_timer_is_active
                ){  _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
                    break;
                }
                Z_clock_time tv_2;
                _gettime( &tv_2 );
                _timersub( &tv_2, &tv, &elapsed_time );
                if( _timercmp( &elapsed_time, <, &suspend_time ) // Czy przeliczanie czasów ‹cyklerów› trwało krócej niż obliczony czas oczekiwania do pierwszego budzącego ‹zadanie›?
                || !some_task_got_ready
                ){  E_base_S->E_flow_Q_timer_S_last_real_time = tv;
                    _timeradd( &tv, &suspend_time, &E_base_S->E_flow_Q_timer_S_next_real_time ); //NDFN Rozważyć przepełnienie licznika czasu rzeczywistego.
                    break;
                }
                tv = tv_2;
                GV_(NA); Gd( suspend_time.tv_sec ); Gd( suspend_time.Z_clock_time_minor_field ); Gd( elapsed_time.tv_sec ); Gd( elapsed_time.Z_clock_time_minor_field ); // Timer schedule loop overhead.
            }
        }
        for_each_out( E_base_S->E_flow_Q_task_S_current, task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
        {   B task_skip = no;
            for_each( task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
            {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
                if( task->run_state == E_flow_Z_run_state_S_stopping_by_task
                && task->run_state_object == task_id
                ) // Nie przełączaj do ‹zadania›, które jest w trakcie wyrzucania innego.
                {   task_skip = yes;
                    break;
                }
            }
            if( !task_skip )
            {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
                    #ifdef E_flow_C_thread_system_unblock_reports
                if( U_R( task->type, system_unblock_report ))
                {   if( task->run_state == E_flow_Z_run_state_S_ready
                    && *task->thread_switch_out
                    )
                    {   E_base_S->E_flow_Q_task_S_current = task_id;
                        pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                        volatile B *thread_switch_in = task->thread_switch_in;
                        volatile B *thread_switch_out = task->thread_switch_out;
                        pthread_cond_t *thread_switch = task->thread_switch;
                        Vr_( pthread_mutex_lock( thread_flow_mutex ));
                        *thread_switch_out = no;
                        *thread_switch_in = yes;
                        Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
                        Vr_( pthread_cond_signal( thread_switch ));
                        Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                        goto Loop; // Wywołuje “E_flow_Q_task_I_schedule” w sposób wewnętrzny, gdy nie można użyć funkcjonalności “E_flow_Q_task_I_switch”, ale przy pierwszej konieczności przełączenia standardowego będzie przełączał w trybie ·pominięcia nie obługiwanych systemowych danych zatrzymywania przepływu wykonania z tablicy ‹zadań›· (pominięcia ‹zadań›) przekazanych kolejnych typu “wątkowanych” ‹systemowych raportów odblokowujących› z “cyklicznej kolejki” — na rzecz zwykłego ‹zadania› (“schedule_task_id”), które wywołało “schedule”.
                    }
                }else
                    #endif
                    #ifdef C_pthreads
                if( U_R( task->type, async ))
                {   if( task->run_state == E_flow_Z_run_state_S_ready
                    && *task->thread_switch_out
                    )
                    {   E_base_S->E_flow_Q_task_S_current = task_id;
                        pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                        volatile B *thread_switch_in = task->thread_switch_in;
                        volatile B *thread_switch_out = task->thread_switch_out;
                        pthread_cond_t *thread_switch = task->thread_switch;
                        Vr_( pthread_mutex_lock( thread_flow_mutex ));
                        *thread_switch_out = no;
                        *thread_switch_in = yes;
                        Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
                        Vr_( pthread_cond_signal( thread_switch ));
                        Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                        goto Loop;
                    }
                }else
                    #endif
                if( task->run_state == E_flow_Z_run_state_S_ready )
                {
                        #ifdef E_flow_C_thread_system_unblock_reports
                    if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                        E_base_S->E_flow_Q_task_S_current = schedule_task_id;
                    if( task_id != E_base_S->E_flow_Q_task_S_current )
                        #endif
                    {   E_flow_Q_task_I_switch( task_id );
                        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
                    }
                    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
                }
            }
        }
        B task_skip = no;
        for_each( task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
        {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
            if( task->run_state == E_flow_Z_run_state_S_stopping_by_task
            && task->run_state_object == E_base_S->E_flow_Q_task_S_current
            )
            {   task_skip = yes;
                break;
            }
        }
        if( !task_skip )
        {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
                #ifdef E_flow_C_thread_system_unblock_reports
            if( U_R( task->type, system_unblock_report ))
            {   if( task->run_state == E_flow_Z_run_state_S_ready
                && *task->thread_switch_out
                )
                {   pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                    volatile B *thread_switch_in = task->thread_switch_in;
                    volatile B *thread_switch_out = task->thread_switch_out;
                    pthread_cond_t *thread_switch = task->thread_switch;
                    Vr_( pthread_mutex_lock( thread_flow_mutex ));
                    *thread_switch_out = no;
                    *thread_switch_in = yes;
                    Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
                    Vr_( pthread_cond_signal( thread_switch ));
                    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                    continue;
                }
            }else
                #endif
                #ifdef C_pthreads
            if( U_R( task->type, async ))
            {   if( task->run_state == E_flow_Z_run_state_S_ready
                && *task->thread_switch_out
                )
                {   pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                    volatile B *thread_switch_in = task->thread_switch_in;
                    volatile B *thread_switch_out = task->thread_switch_out;
                    pthread_cond_t *thread_switch = task->thread_switch;
                    Vr_( pthread_mutex_lock( thread_flow_mutex ));
                    *thread_switch_out = no;
                    *thread_switch_in = yes;
                    Vr_( pthread_cond_wait( thread_switch, thread_flow_mutex ));
                    Vr_( pthread_cond_signal( thread_switch ));
                    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                    continue;
                }
            }else
                #endif
            if( task->run_state == E_flow_Z_run_state_S_ready )
            {
                    #ifdef E_flow_C_thread_system_unblock_reports
                if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                    E_base_S->E_flow_Q_task_S_current = schedule_task_id;
                    #endif
                return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
            }
        }
        if( U_R( E_base_S->E_flow_S_signal, exit )
        || U_R( E_base_S->E_flow_S_signal, time )
        || U_R( E_base_S->E_flow_S_signal, call_req )
        || U_R( E_base_S->E_flow_S_signal, io_ready )
        )
            continue;
        if( _timercmp( &E_base_S->E_flow_Q_timer_S_next_real_time, <, &next_real_time ))
            next_real_time = E_base_S->E_flow_Q_timer_S_next_real_time;
        B U_has_suspend_time = !_timerisover( &next_real_time );
        if( U_has_suspend_time )
            _gettime( &tv );
        if( !U_has_suspend_time
        || _timercmp( &tv, <, &next_real_time )
        ){  if( U_has_suspend_time )
            {   _timersub( &next_real_time, &tv, &tv );
                    #ifdef E_flow_C_itimer_system_unblock_report
                if( E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer )
                    E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer( &tv );
                else
                    #endif
                    _setttimer( &tv );
            }
            _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_volatile, 0 );
                #ifdef E_flow_C_itimer_system_unblock_report
            if( E_base_S->E_flow_Z_itimer_system_unblock_report_I_sigsuspend )
            {   B system_unblock_report = no;
                while( !U_R( E_base_S->E_flow_S_signal, wake ))
                {   system_unblock_report = E_base_S->E_flow_Z_itimer_system_unblock_report_I_sigsuspend( &E_base_S->E_flow_Z_sigset_S_empty );
                    if( system_unblock_report )
                        U_F( E_base_S->E_flow_S_signal, wake );
                    else
                        if( U_has_suspend_time
                        && E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer
                        ){  _gettime( &tv );
                            if( _timercmp( &tv, <, &next_real_time )) // Ustaw pozostały czas.
                            {   _timersub( &next_real_time, &tv, &tv );
                                E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer( &tv );
                            }else // Lub był upłynął.
                                U_F( E_base_S->E_flow_S_signal, wake );
                        }
                }
                if( system_unblock_report )
                {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Z_itimer_system_unblock_report_S_task_id );
                    task->run_state = E_flow_Z_run_state_S_ready;
                }
            }else
                #endif
                while( !U_R( E_base_S->E_flow_S_signal, wake ))
                {   Vp_(( sigsuspend( &E_base_S->E_flow_Z_sigset_S_empty ), !_errno || _errno == EINTR ));
                }
            if( U_has_suspend_time
            && !U_R( E_base_S->E_flow_S_signal, time )
            ){  Z_clock_time tv_;
                _timerclear( &tv_ );
                    #ifdef E_flow_C_itimer_system_unblock_report
                if( E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer )
                    E_base_S->E_flow_Z_itimer_system_unblock_report_I_setitimer( &tv_ );
                else
                    #endif
                    _setttimer( &tv_ );
            }
            U_L( E_base_S->E_flow_S_signal, wake );
            _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_volatile, 0 );
        }
    }
}
// Przełączenie do ‹zadania› przez przełączenie wskaźnika “stosu” wykonania.
_internal
__attribute__ (( __noinline__, __returns_twice__, __hot__ ))
void
E_flow_Q_task_I_switch( I task_to_id
){  Vr_( feclearexcept( FE_ALL_EXCEPT ));
    struct E_flow_Q_task_Z *task_from = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    struct E_flow_Q_task_Z *task_to = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_to_id );
    E_base_S->E_flow_Q_task_S_current = task_to_id;
    __asm__ volatile (
        #if defined( __i386__ ) || defined( __x86_64__ )
            #if defined( __i386__ )
    "\n" "mov       %%esp,%0"
    "\n" "test      %1,%1"
//CONF jest ‘cmov’?
                #if 1
    "\n" "cmovnz    %1,%%esp"
                #else
    "\n" "jz        0f"
    "\n" "mov       %1,%%esp"
    "\n" "0:"
                #endif
            #else
    "\n" "mov       %%rsp,%0"
    "\n" "test      %1,%1"
    "\n" "cmovnz    %1,%%rsp"
            #endif
    : "=m" ( task_from->exe_stack )
    : "r" ( task_to->exe_stack )
    : "cc", "memory"
//CONF Jest FPU?
    , "st", "st(1)", "st(2)", "st(3)", "st(4)", "st(5)", "st(6)", "st(7)"
            #ifdef __MMX__
    , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
                #ifdef __SSE__
    , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
                #endif
            #endif
            #if defined( __i386__ )
    , "ebx", "ecx", "esi", "edi"
            #else
    , "rbx", "rcx", "rsi", "rdi"
    , "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
                #ifdef __SSE3__
    , "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14", "xmm15"
                    #ifdef __AVX__
    , "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "ymm6", "ymm7", "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13", "ymm14", "ymm15"
                    #endif
                #endif
            #endif
        #else
#error not implemented
        #endif
    );
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_internal
void
E_flow_Z_signal_V_chld( int uid
){  Da_();
    int errno_ = _errno;
    int status;
    VOp_(( ~waitpid( -1, &status, WNOHANG ) || _errno == ECHILD ));
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_term( int uid
){  Da_();
    int errno_ = _errno;
    struct sigaction sa =
    { .sa_handler = SIG_IGN
    , .sa_flags = 0
    };
    V0_( sigaction( SIGHUP, &sa, 0 ));
    V0_( sigaction( SIGINT, &sa, 0 ));
    V0_( sigaction( SIGQUIT, &sa, 0 ));
    V0_( sigaction( SIGTERM, &sa, 0 ));
    U_F( E_base_S->E_flow_S_signal, exit );
    U_F( E_base_S->E_flow_S_signal, wake );
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_segv( int uid
, siginfo_t *siginfo
, P context
){  Da_();
    int errno_ = _errno;
    if( E_base_S->E_flow_Q_task_S_current )
    {   if( U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
        {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
            if( (Pc)siginfo->si_addr < task->touched_stack )
            {   if( (Pc)siginfo->si_addr >= task->stack + E_base_S->E_mem_S_page_size )
                {   Pc new_base = E_simple_Z_p_I_align_down_to_v2( siginfo->si_addr, E_base_S->E_mem_S_page_size );
                    V0_( mprotect( new_base
                    , task->touched_stack - new_base
                    , PROT_READ | PROT_WRITE
                    ));
                    task->touched_stack = new_base;
                    _errno = errno_;
                    return;
                }
                if( (Pc)siginfo->si_addr >= task->stack )
                {   GV_(NA); // Late instrumentation error: task stack too small.
                }
            }
        }else
            if( (Pc)siginfo->si_addr < E_base_S->E_flow_Q_task_S_sigsegv_tmp.touched_stack )
            {   if( (Pc)siginfo->si_addr >= E_base_S->E_flow_Q_task_S_sigsegv_tmp.stack + E_base_S->E_mem_S_page_size )
                {   Pc new_base = E_simple_Z_p_I_align_down_to_v2( siginfo->si_addr, E_base_S->E_mem_S_page_size );
                    V0_( mprotect( new_base
                    , E_base_S->E_flow_Q_task_S_sigsegv_tmp.touched_stack - new_base
                    , PROT_READ | PROT_WRITE
                    ));
                    E_base_S->E_flow_Q_task_S_sigsegv_tmp.touched_stack = new_base;
                    _errno = errno_;
                    return;
                }
                if( (Pc)siginfo->si_addr >= E_base_S->E_flow_Q_task_S_sigsegv_tmp.stack )
                {   GV_(NA); // Late instrumentation error: task stack too small.
                }
            }
    }
    GV_(NDFN); Gd( siginfo->si_code ); Gh( siginfo->si_addr ); // Other fault.
    struct sigaction sa =
    { .sa_handler = SIG_DFL
    , .sa_flags = 0
    };
    V0_( sigaction( SIGSEGV, &sa, 0 ));
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_alrm( int uid
, siginfo_t *siginfo
, P data
){  Da_();
    int errno_ = _errno;
    if( !siginfo->si_pid ) //NDFN Brak w dokumentacji, ale tylko to jest stałe pomiędzy systemami dla “sygnału” z “setitimer” lub “timer_settime”.
        U_F( E_base_S->E_flow_S_signal, time );
    U_F( E_base_S->E_flow_S_signal, wake );
    _errno = errno_;
}
//------------------------------------------------------------------------------
_internal
void
E_flow_Z_signal_V_process_call_ping( int uid
, siginfo_t *siginfo
, P data
){  Da_();
    if( siginfo->si_code != SI_USER )
        return;
    int errno_ = _errno;
    V0( kill( siginfo->si_pid, SIGPROF )){}
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_pong( int uid
, siginfo_t *siginfo
, P data
){  Da_();
    if( siginfo->si_code != SI_USER )
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   U_L( call->state, ping );
            U_F( E_base_S->E_flow_S_signal, wake );
            break;
        }
    }
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_req( int uid
, siginfo_t *siginfo
, P data
){  Da_();
        #ifndef E_flow_Q_process_call_C_alt
    if( siginfo->si_code != SI_QUEUE )
        #else
    if( siginfo->si_code != SI_USER )
        #endif
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_srv_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   GV_(NXP); Gd( siginfo->si_pid ); // Duplicate process call request.
            _errno = errno_;
            return;
        }
    }
    P shm;
        #ifndef E_flow_Q_process_call_C_alt
    if( !~(N)( shm = E_mem_Q_shared_Q_blk_M( siginfo->si_value.sival_int )))
        #else
    N l = E_text_Z_n_N_s_G( siginfo->si_pid, sizeof( siginfo->si_pid ), 10 );
    C s[ 6 + l + 1 ];
    E_text_Z_s_P_copy_s0( s, "/proc/" );
    E_text_Z_n_N_s( &s[0] + 6 + l, siginfo->si_pid, sizeof( siginfo->si_pid ), 10 );
    *( &s[0] + 6 + l ) = '\0';
    key_t key;
    V1( key = ftok( s, E_flow_Q_process_call_S_ftok_id ))
    {   _errno = errno_;
        return;
    }
    int shm_id;
    if( !~( shm_id = E_mem_Q_shared_R(key))
    || !~(N)( shm = E_mem_Q_shared_Q_blk_M( shm_id ))
    )
        #endif
    {   _errno = errno_;
        return;
    }
    call_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_process_call_srv_S );
    struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
    call->process_id = siginfo->si_pid;
    call->shm = shm;
    U_F( E_base_S->E_flow_S_signal, call_req );
    U_F( E_base_S->E_flow_S_signal, wake );
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_reply( int uid
, siginfo_t *siginfo
, P data
){  Da_();
        #ifndef E_flow_Q_process_call_C_alt
    if( siginfo->si_code != SI_QUEUE )
        #else
    if( siginfo->si_code != SI_USER )
        #endif
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   U_L( call->state, active );
            U_L( call->state, ping );
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, call->task_id );
            task->run_state = E_flow_Z_run_state_S_ready;
            U_F( E_base_S->E_flow_S_signal, wake );
            _errno = errno_;
            return;
        }
    }
    GV_(NXP); Gd( siginfo->si_pid ); // Call reply from a unrequested process.
    _errno = errno_;
}
//------------------------------------------------------------------------------
    #if defined( E_io_C_aio ) || defined( E_io_Q_stream_out_C_sig )
_internal
void
E_flow_Z_signal_V_io_ready( int uid
, siginfo_t *siginfo
, P data
){  Da_();
    int errno_ = _errno;
    //if( siginfo->si_code != POLL_OUT )
    //{   G_(); Gh( siginfo->si_code ); Gd( siginfo->si_fd );
    //}
        #if defined( E_io_C_aio ) || defined( E_io_C_sig_has_fd )
            #ifdef E_io_C_aio
    if( siginfo->si_code != SI_ASYNCIO )
    {   _errno = errno_;
        return;
    }
            #else
    //if( siginfo->si_code != POLL_OUT
    //){  struct sigaction sa =
        //{ .sa_handler = SIG_IGN
        //, .sa_flags = 0
        //};
        //V0_( sigaction( SIGHUP, &sa, 0 ));
        //V0_( sigaction( SIGINT, &sa, 0 ));
        //V0_( sigaction( SIGQUIT, &sa, 0 ));
        //V0_( sigaction( SIGTERM, &sa, 0 ));
        //U_F( E_base_S->E_flow_S_signal, exit );
        //U_F( E_base_S->E_flow_S_signal, wake );
        //_errno = errno_;
        //return;
    //}
            #endif
    B found = no;
    for_each( id, E_base_S->E_io_Q_stream_out_S, E_mem_Q_tab )
    {   struct E_io_Q_stream_out_Z *stream = E_mem_Q_tab_R( E_base_S->E_io_Q_stream_out_S, id );
            #ifdef E_io_C_aio
        if( stream->aiocb->aio_fildes == siginfo->si_value.sival_int )
        {   if( !U_R( stream->state, closing ))
            {   U_F( stream->state, io_ready );
                found = yes;
            }
            break;
        }
            #else
        if( stream->aiocb->aio_fildes == siginfo->si_fd )
        // Jeśli okaże się (a jest to bardzo prawdopodobne), że jest generowany “sygnał” tylko dla jednego strumienia na “terminalu”, to trzeba odpowiednio odkomentować poniższe linie.
        //{   if( stream->tty )
            //{   Pc tty = stream->tty;
                //for_each( id, E_base_S->E_io_Q_stream_out_S, E_mem_Q_tab )
                //{   struct E_io_Q_stream_out_Z *stream = E_mem_Q_tab_R( E_base_S->E_io_Q_stream_out_S, id );
                    //if( !*stream->tty )
                        //continue;
                    {if( !U_R( stream->state, closing )
                    //&& E_text_Z_s0_T_eq_s0( stream->tty, tty )
                    && ( stream->aiocb->aio_nbytes
                      || stream->data_l
                    ))
                    {   U_F( stream->state, io_ready );
                        found = yes;
                    }
                //}
            //}else
            //{   U_F( stream->state, io_ready );
                //found = yes;
            //}
            break;
        }
            #endif
    }
    if(found)
        #endif
    {   U_F( E_base_S->E_flow_S_signal, io_ready );
        U_F( E_base_S->E_flow_S_signal, wake );
    }
    _errno = errno_;
}
#endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_export
I
E_flow_Q_process_call_M( N l
, P *data
){  N shm_id;
        #ifndef E_flow_Q_process_call_C_alt
    if( !~( shm_id = E_mem_Q_shared_M(l)))
        #else
    if( !~( shm_id = E_mem_Q_shared_M_key( E_base_S->E_flow_Q_process_call_S_shm_key, l )))
        #endif
        return ~0;
    P shm;
    if( !~(N)( shm = E_mem_Q_shared_Q_blk_M( shm_id )))
    {   E_mem_Q_shared_W( shm_id );
        return ~0;
    }
    _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    I call_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_process_call_cli_S );
    struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
    U_L( call->state, active );
    call->shm_id = shm_id;
    call->shm = shm;
    _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    *data = shm;
    return call_id;
}
_export
N
E_flow_Q_process_call_W( I id
){  struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, id );
    E_mem_Q_shared_Q_blk_W( call->shm );
    E_mem_Q_shared_W( call->shm_id );
    _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    N r = E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_process_call_cli_S, id );
    _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    return r;
}
//------------------------------------------------------------------------------
_export
B
E_flow_Q_process_call_I( I id
, pid_t process_id
, B *successful
){  J_assert( process_id > 0 );
    struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, id );
    call->task_id = E_base_S->E_flow_Q_task_S_current;
    call->process_id = process_id;
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_call_reply;
    _gettime( &task->run_state_time );
    _timeradd( &task->run_state_time, &E_base_S->E_flow_Q_process_call_S_ping_period, &task->run_state_time );
        #ifndef E_flow_Q_process_call_C_alt
    union sigval sv;
    sv.sival_int = call->shm_id;
    U_F( call->state, active );
    V0( sigqueue( process_id, SIGUSR1, sv ))
        #else
    V0( kill( process_id, SIGUSR1 ))
        #endif
    {   U_L( call->state, active );
        *successful = no;
        return no;
    }
    B r = E_flow_Q_task_I_schedule();
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    *successful = !U_R( call->state, ping );
    return r;
}
//------------------------------------------------------------------------------
D( flow, call_srv )
{   X_M_( flow, call_req );
    if( _X_var( flow, call_req ) < 0 )
        U_F( E_base_S->E_flow_S_signal, exit );
    O{  X_B( flow, call_req, 0 )
            break;
        _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_req, 0 )
        {   U_F( E_base_S->E_flow_S_signal, exit );
            continue;
        }
        for_each_pop( call_id, E_base_S->E_flow_Q_process_call_srv_S, E_mem_Q_tab )
        {   struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
            E_flow_Q_process_call_I_func( call->process_id, call->shm );
            E_mem_Q_shared_Q_blk_W( call->shm );
                #ifndef E_flow_Q_process_call_C_alt
            union sigval sv;
            V0( sigqueue( call->process_id, SIGUSR2, sv )){}
                #else
            V0( kill( call->process_id, SIGUSR2 )){}
                #endif
        }
        _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_req, 0 )
            U_F( E_base_S->E_flow_S_signal, exit );
    }
    X_W( flow, call_req );
}
/******************************************************************************/
