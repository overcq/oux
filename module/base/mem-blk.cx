//-*-C-*-
/*******************************************************************************
*   ___   publicplace
*  ¦OUX¦  C+
*  ¦/C+¦  component
*   ---   base
*         memory blocks manager
* ©overcq                on ‟Gentoo Linux 13.0” “x86_64”             2015‒4‒28 *
*******************************************************************************/
/** Menedżer bloków pamięci alokowanej dynamicznie, ale nie do niepotrzebnego alokowania i zwalniania w pętlach, przechowujący rozmiar pojedynczego elementu bloku tylko do własnej administracji pamięcią.
Implementacyjnie– w “E_mem_Q_blk” uchwytami są adresy elementów (ponad zmienianymi numerami elementów), a w “E_mem_Q_tab”– numery elementów (ponad zmienianymi adresami). Czyli w sensie funkcjonalnym z zewnątrz– ten pierwszy jest menedżerem tablic rzeczywistych (ciągłych w pamięci), a ten drugi– zarządzanych.
⁂
Deterministyczna strategia modyfikacji pamięci stosowana w tym menedżerze i globalnie wewnętrznie w miejscu użycia jego funkcji.
Struktura osadzenia przepływu wykonania menedżera:
⁃ W punkcie wejścia oraz wyjścia przepływu wykonania ‹zadania› z funkcji menedżera stan pamięci zarządzanej jest integralny. Funkcja może zmienić adres tylko tego bloku pamięci spośród udostępnionych przez menedżera, względem którego została wywołana.
⁃ W trakcie posiadania przepływu wykonania przez menedżera może wystąpić przerwanie zabierające przepływ wykonania— w postaci “sygnału” ‘uniksowego’, którego procedura obsługi może ewentualnie wykonywać dostęp do bloku pamięci zarządzanego przez tego menedżera; teksty procedur obsługi są w “menedżerze przepływu wykonania”.
⁃ Jednak “sygnały” ‘uniksowe’ czytające lub zmieniające konkretne bloki pamięci są blokowane przed przejęciem przepływu wykonania podczas ich modyfikacji, ponieważ te konkretne bloki są używane poprzez któregoś menedżera ·struktur· pamięci (menedżera pamięci wyższego poziomu), który nie potrafiłby wykonać swoich funkcji “atomowo” ze względu na złożoność danych, więc “sygnał” ‘uniksowy’ nie wystąpi podczas zmiany adresu bloku pamięci, z którego korzysta.
⁃ Jakkolwiek “sygnał” ‘uniksowy’ “SEGV” używany nie jako błąd —do automatycznego ‘doalokowywania’ stron pamięci “stosów” ‹zadań› z wcześniej zarezerwowanego obszaru przestrzeni adresowej— może wystąpić kiedykolwiek po przesunięciu niżej wierzchołka “stosu” ‹zadania› w procedurze tego menedżera przez ‘kompilator’, a procedura obsługi tego “sygnału” czyta i zmienia dane w tablicy ‹zadań›. Więc w konkretnej implementacji zmieniania tablicy ‹zadań› “stos” ‹zadania› jest ‘prealokowany’ (“E_flow_Q_task_I_touch_stack”), a dla innego “thread”– ustawiany ‹przełącznik› zabraniający czytania tej tablicy w procedurze obsługi “sygnału” “SEGV”.
Struktura “atomowości” menedżera:
⁃ W tekście programu menedżera jest gwarantowane ‘implicite’ ze składni języka (tzn. z domyślnej alokacji zmiennych lokalnych na “stosie” ·przed ich użyciem· jako wartość wyrażenia przypisywanego), że wpis w tablicy systemowej menedżera (adres, rozmiar bloku) zostanie zmieniony “atomowo” względem poszczególnego elementu wpisu (adres lub rozmiar), oraz z linii programu– zawsze tak, by przed i po “atomowej” zmianie odczyt całego wpisu był poprawny względem chwilowo obecnego bloku pamięci z zawartością. To umożliwia przerwanie przepływu wykonania przez “sygnał” lub “wątek” systemu operacyjnego i poprawny dostęp do pamięci udostępnionej przez menedżera, ale nie wywołanie funkcji menedżera. (Czy to ma jakieś zastosowanie, gdy funkcje i tak muszą być blokowane?)
⁃ Pomiędzy wszystkimi tablicami systemowymi menedżera integralność jest zachowywana przez realizację właściwej sekwencji zmieniania wpisów, w których to sekwencjach blok pamięci jest utrzymywany w pierwszej funkcji na nim się wykonującej— przez utrzymywanie jego wpisu w zmiennych lokalnych, a usuniętego z tablicy wolnych bloków, niedostępnego dla kolejnych funkcji. (Kolejność zmian “atomowych” dla chwilowej poprawności w trakcie zmiany pojedynczego wpisu nie została jeszcze sprawdzona.) To umożliwia rekurencyjne wywoływanie uniwersalnych funkcji wewnętrz menedżera oraz po dodaniu odpowiednich instrukcji synchronizacji “międzywątkowych” umożliwi przerywanie przepływu wykonania menedżera przez system operacyjny “wywłaszczający” dla kolejnego “wątku” spośród tych (nie synchronizowanych w “menedżerze przepływu wykonania”) nałożonych na program w technologii ‟oux” wymaganiami którejś ‘dokompilowanej’ “biblioteki”— i wykonanie funkcji menedżera przez program “biblioteczny” (np. zastąpionej procedury “biblioteki” ‟C”: “malloc”, “calloc”, “realloc”, “free”).
⁃ Natomiast kopiowanie zawartości bloku pamięci podczas ‘realokacji’ obejmujące zwalnianie pamięci odrazu do systemu operacyjnego lub obszary zachodzące musi być realizowane tak, by nie nastąpiło przerwanie, a w szczególności– przez “SIGSEGV”.
⁂
Można alokować zero jednostek pamięci: ‘alokowany’ jest symboliczny adres będący małą liczbą, traktowany tak samo jak adres rzeczywisty.
Parametry podawane do funkcji nie mogą oznaczać pustych operacji; jest to wymagane dla niewyjścia poza integralność programu.
Funkcje podają “0” przy braku możliwosci ‘alokacji’ nowej pamięci, a przy nieznalezieniu wpisu w rejestrze ‘alokowanych’ bloków generują ‹niepowodzenie zakańczające›.
⁂
Zależnie od zastosowania powiększania bloku pamięci, w celu niewykonywania niepotrzebnych instrukcji należy wybrać pierwszą funkcję w kolejności: “add” (nie zależy, gdzie dodane), [“prepend_append”], “append”, “prepend”, “insert” (gdy musi być konkretne miejsce).
*/
//------------------------------------------------------------------------------
//TODO mapped free
//TODO Sprawdzić, czy gwarantuje poprawne ‘alokowanie’ bloków pamięci przekraczających 0. Nie należy przekraczać 0, ponieważ jest ono używane jako wartość niepoprawnego adresu w systemie operacyjnym i strona o tym adresie nie jest ‘alokowana’, ale ostatnia strona przestrzeni adresowej może być (~0 ma znaczenie tylko dla ‹identyfikatorów›, a inne to tylko w systemie operacyjnym).
//TODO Funkcje “move” oprócz obecnych “copy”, by można było w nich użyć “remap”, jeśli jest dostępne.
// Funkcja porządkująca tablice w dostępnym nadmiarowo czasie wykonania? Obecnie nic aktywnie nie porządkuje ani tablic systemowych, ani pamięci bloków, a tylko pasywnie podczas wykonywania żądanych funkcji, jeśli coś jest oczywiste w wykonywaniu funkcji.
// Zaimplementować ‘alokacje’ sfragmentowane (listy bloków ‘alokowanego’) i optymalizacje przy braku pamieci z projektowanego kiedyś menedżera pamięci (“mem_mng– use”).
//==============================================================================
    #ifdef C_to_libs_C_replace_c_alloc
        #ifdef C_pthreads
#define _single_thread_begin    Vr_( pthread_mutex_lock( &E_base_S->E_mem_Q_blk_S_threads_sync_mutex ))
#define _single_thread_end      Vr_( pthread_mutex_unlock( &E_base_S->E_mem_Q_blk_S_threads_sync_mutex ))
        #else
#define _single_thread_begin
#define _single_thread_end
        #endif
    #else
#define _single_thread_begin
#define _single_thread_end
    #endif
//------------------------------------------------------------------------------
#define E_mem_Q_blk_S_align_to_all  ( 2 * sizeof(N) )
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct E_mem_Q_blk_Z_mapped // Obszary stron pamięci otrzymane z systemu operacyjnego.
{ Pc p; // “0” tutaj i w “l” oznacza pusty wpis; ale wystarczy sprawdzić jedno.
  N l;
};
struct E_mem_Q_blk_Z_free // Dla optymalizacji wyszukiwania obszaru nowej ‘alokacji’.
{ Pc p; // “0” tutaj i w “l” oznacza pusty wpis; ale wystarczy sprawdzić jedno.
  N l;
};
struct E_mem_Q_blk_Z_allocated // Dla kontroli poprawności wydawania i oddawania ‘alokacji’.
{ Pc p; // “0” oznacza pusty wpis.
  N u; // ‘unit’
  N n; // W niepustym wpisie— “0” oznacza ‛zerowy wpis’.
};
//==============================================================================
    #ifdef E_mem_Q_blk_C_debug
#define T_sign(v) ( (S)(v) < 0 )
_internal
B
E_mem_Q_blk_T_new_0( P p
){  Pc p_end;
    V1p_( p_end = sbrk(0) );
    return (Pc)p < p_end;
}
//NDFN Potrzebne jest utworzenie nowej sekwencji testów — kompletnych.
_internal
void
E_mem_Q_blk_I_assert_on_return( N line
){  sigset_t sigset;
    _sigprocmask( SIG_SETMASK, &E_base_S->E_flow_Z_sigset_S_process_exec, &sigset );
    B b = U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read );
    if(b)
    {   E_flow_Q_task_I_touch_stack();
        U_L( E_base_S->E_flow_S_mode, Z_task_table_S_can_read );
    }
    struct E_mem_Q_blk_Z_mapped *mapped_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_mapped_id ].p;
    struct E_mem_Q_blk_Z_allocated *allocated_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].p;
    struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
    N mapped_n = E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_mapped_id ].n;
    N allocated_n = E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n;
    N free_n = E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n;
    for_n( j, allocated_n )
        if( allocated_p[j].p
        && E_mem_Q_blk_T_new_0( allocated_p[j].p )
        )
        {   for_n( i, allocated_n )
            {   if( i == j )
                    continue;
                if( allocated_p[i].p == allocated_p[j].p )
                {   G_(); Gd(line); Gh( allocated_p[j].p ); V(); // duplicate
                }
            }
        }
    for_n_( j, free_n )
    {   if(( !free_p[j].p
          && free_p[j].l
        )
        || ( free_p[j].p
          && !free_p[j].l
        ))
        {   G_(); Gd(line); Gh( free_p[j].p ); Gd( free_p[j].l ); V(); // inconsistent
        }
        if( free_p[j].l > 64 * 1024 ) //NDFN
        {   G_(); Gd(line); Gh( free_p[j].p ); Gd( free_p[j].l ); V(); // probably invalid value
        }
    }
    for_n_( j, mapped_n )
    {   Pc p = mapped_p[j].p;
        if( !E_simple_Z_p_T_aligned_to_v2( p, E_base_S->E_mem_S_page_size ))
        {   G_(); Gd(line); Gh(p); Gh(p); V();
        }
        if(( !mapped_p[j].p
          && mapped_p[j].l
        )
        || ( mapped_p[j].p
          && !mapped_p[j].l
        ))
        {   G_(); Gd(line); Gh(p); V(); // inconsistent
        }
        if( !mapped_p[j].p )
            continue;
        if( !E_simple_Z_n_T_aligned_to_v2( mapped_p[j].l, E_base_S->E_mem_S_page_size ))
        {   G_(); Gd(line); Gh(p); Gh( mapped_p[j].l ); V();
        }
        do
        {   for_n( i, allocated_n )
            {   if( !allocated_p[i].p
                || !allocated_p[i].n
                )
                    continue;
                if( p == allocated_p[i].p )
                {   p += allocated_p[i].n * allocated_p[i].u;
                    allocated_p[i].n = ~allocated_p[i].n;
                    break;
                }
            }
            if( i == allocated_n )
            {   for_n_( i, free_n )
                {   if( !free_p[i].p )
                        continue;
                    if( p == free_p[i].p )
                    {   p += free_p[i].l;
                        free_p[i].l = ~free_p[i].l;
                        break;
                    }
                }
                if( i == free_n )
                {   G_(); Gd(line); Gh(p); V(); // linearity
                }
            }
        }while( p < mapped_p[j].p + mapped_p[j].l );
        if( p != mapped_p[j].p + mapped_p[j].l )
        {   G_(); Gd(line); Gh(p); V(); // not mapped to end
        }
    }
    for_n_( j, allocated_n )
    {   if( !allocated_p[j].p
        || !allocated_p[j].n
        )
            continue;
        if( !T_sign( allocated_p[j].n ))
        {   G_(); Gd(line); Gh( allocated_p[j].p ); V(); // lost
        }
        allocated_p[j].n = ~allocated_p[j].n;
    }
    for_n_( j, free_n )
    {   if( !free_p[j].p )
            continue;
        if( !T_sign( free_p[j].l ))
        {   G_(); Gd(line); Gh( free_p[j].p ); Gd( free_p[j].l ); V(); // lost
        }
        free_p[j].l = ~free_p[j].l;
    }
    if(b)
        U_F( E_base_S->E_flow_S_mode, Z_task_table_S_can_read );
    _sigprocmask( SIG_SETMASK, &sigset, 0 );
}
    #else
#define E_mem_Q_blk_I_assert_on_return(line)
    #endif
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
B
E_mem_Q_blk_T_eq( P p_1
, P p_2
, N l
){  for_n( i, l )
        if( *( (Pc)p_1 + l ) != *( (Pc)p_2 + l ))
            return no;
    return yes;
}
P
E_mem_Q_blk_I_copy( P dst
, P src
, N l
){  J_assert( (Pc)dst < (Pc)src || (Pc)dst >= (Pc)src + l );
    Pn dst_n = (P)E_simple_Z_p_I_align_up_to_v2( dst, sizeof(N) );
    Pn src_n = (P)E_simple_Z_p_I_align_up_to_v2( src, sizeof(N) );
    if( (Pc)src + l >= (Pc)src_n
    && (Pc)dst_n - (Pc)dst == (Pc)src_n - (Pc)src
    )
    {   N l_0 = (Pc)src_n - (Pc)src;
        N l_1 = ( l - l_0 ) / sizeof(N);
        N l_2 = ( l - l_0 ) % sizeof(N);
        Pc dst_c = dst, src_c = src;
        for_n( i, l_0 )
        {   *dst_c = *src_c;
            dst_c++;
            src_c++;
        }
        for_n_( i, l_1 )
        {   *dst_n = *src_n;
            dst_n++;
            src_n++;
        }
        dst_c = (Pc)dst_n;
        src_c = (Pc)src_n;
        for_n_( i, l_2 )
        {   *dst_c = *src_c;
            dst_c++;
            src_c++;
        }
        return dst_c;
    }
    Pc dst_c = dst, src_c = src;
    for_n( i, l )
    {   *dst_c = *src_c;
        dst_c++;
        src_c++;
    }
    return dst_c;
}
P
E_mem_Q_blk_I_copy_rev( P dst
, P src
, N l
){  J_assert( (Pc)dst > (Pc)src || (Pc)dst <= (Pc)src - l );
    Pn dst_n = (P)E_simple_Z_p_I_align_down_to_v2( (Pc)dst + l, sizeof(N) );
    Pn src_n = (P)E_simple_Z_p_I_align_down_to_v2( (Pc)src + l, sizeof(N) );
    if( (Pc)src <= (Pc)src_n
    && (Pc)dst + l - (Pc)dst_n == (Pc)src + l - (Pc)src_n
    )
    {   N l_0 = (Pc)src + l - (Pc)src_n;
        N l_1 = ( l - l_0 ) / sizeof(N);
        Pc dst_c = (Pc)dst + l, src_c = (Pc)src + l;
        for_n( i, l_0 )
        {   dst_c--;
            src_c--;
            *dst_c = *src_c;
        }
        for_n_( i, l_1 )
        {   dst_n--;
            src_n--;
            *dst_n = *src_n;
        }
        dst_c = (Pc)dst_n;
        src_c = (Pc)src_n;
        while( src_c != src )
        {   dst_c--;
            src_c--;
            *dst_c = *src_c;
        }
        return dst_c;
    }
    Pc dst_c = (Pc)dst + l, src_c = (Pc)src + l;
    while( src_c != src )
    {   dst_c--;
        src_c--;
        *dst_c = *src_c;
    }
    return dst_c;
}
P
E_mem_Q_blk_I_copy_auto( P dst
, P src
, N l
){  if( (Pc)dst < (Pc)src
    || (Pc)dst >= (Pc)src + l
    )
        return E_mem_Q_blk_I_copy( dst, src, l );
    return E_mem_Q_blk_I_copy_rev( dst, src, l );
}
//TODO Do optymalizacji?
P
E_mem_Q_blk_P_fill_c( P p
, N l
, C c
){  Pc s = p;
    for_n( i, l )
        *s++ = c;
    return s;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_internal
B
E_mem_Q_blk_Q_sys_table_mf_I_unite( N table_i
, N rel_addr_p
, N rel_addr_l
, P p
, N l
){  N i_found = ~0;
    for_n( i, E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ) // Szukanie bloku przyległego od dołu.
    {   Pc *p_ = (P)( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p );
        N *l_ = (P)( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l );
        if( *p_ + *l_ == p )
        {   p = *p_;
            l = *l_ += l;
            i_found = i;
            break;
        }
    }
    for_n_( i, E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ) // Szukanie bloku przyległego od góry.
    {   Pc *p_ = (P)( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p );
        N *l_ = (P)( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l );
        if( (Pc)p + l == *p_ )
        {   if( ~i_found ) // Był znaleziony blok przyległy od dołu.
            {   *( Pc * )( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + i_found * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) += *l_;
                *p_ = 0;
                *l_ = 0;
            }else
            {   *p_ = p;
                *l_ += l;
                i_found = i;
            }
            break;
        }
    }
    return !!~i_found;
}
_internal
N
E_mem_Q_blk_Q_sys_table_mf_P_put( N table_i
, N rel_addr_p
, N rel_addr_l
, P p
, N l
){  if( !E_mem_Q_blk_Q_sys_table_mf_I_unite( table_i, rel_addr_p, rel_addr_l, p, l ))
    {   if( table_i == E_mem_Q_blk_S_allocated_S_free_id
        && l >= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u
        )
        {   if( (Pc)p + l == E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p ) // Nowy blok jest przyległy od dołu do tablicy bloków.
            {   E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p -= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p;
                free_p[0].l = l - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                free_p[0].p = free_p[0].l ? p : 0;
                return 0;
            }
            if( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p + E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u == p ) // Nowy blok jest przyległy od góry do tablicy bloków.
            {   struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p;
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ].l = l - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ].p = free_p[ E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ].l
                  ? (Pc)p + E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u
                  : 0;
                E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                return 0;
            }
        }
        N i = E_mem_Q_blk_Q_sys_table_R_new_id( table_i, rel_addr_p, rel_addr_l, p, l );
        if( !~i )
            return ~0;
    }
    return 0;
}
_internal
N
E_mem_Q_blk_Q_sys_table_R_new_id( N table_i
, N rel_addr_p
, N rel_addr_l
, P p // Adres do nowego wpisu.
, N l // I rozmiar.
){  Pc p_0 = E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p;
    N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
    if( table_i == E_mem_Q_blk_S_allocated_S_mapped_id
    || table_i == E_mem_Q_blk_S_allocated_S_free_id
    )
    {   for_n( i, E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ) // Szukanie wolnego wpisu w tablicy.
            if( !*( P * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ))
            {   *( N * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
                *( P * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
                return i;
            }
        struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
        N l_ = E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
        for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
            if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
            {   if( free_p[ free_i ].l >= l_ )
                {   free_p[ free_i ].l -= l_;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
                        *( N * )( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
                    *( P * )( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
                    E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p -= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                    E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                    if( table_i == E_base_S->E_mem_Q_blk_S_table_allocated_id )
                    {   E_base_S->E_mem_Q_blk_S_allocated = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].p;
                        E_base_S->E_mem_Q_blk_S_table_allocated_id++;
                    }
                    return 0;
                }
                break;
            }
        for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
            if( p_0 + l_0 == free_p[ free_j ].p )
            {   if( free_p[ free_j ].l >= l_ )
                {   free_p[ free_j ].l -= l_;
                    if( free_p[ free_j ].l )
                        free_p[ free_j ].p += l_;
                    else
                        free_p[ free_j ].p = 0;
                    if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
                        *( N * )( p_0 + l_0 + rel_addr_l ) = l;
                    *( P * )( p_0 + l_0 + rel_addr_p ) = p;
                    return E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                }
                break;
            }
    }else
    {   for_n( i, E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ) // Szukanie wolnego wpisu w tablicy.
            if( !*( P * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ))
            {   if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
                    *( N * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
                *( P * )( p_0 + i * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
                return i;
            }
        struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
        for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
            if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
            {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u )
                {   free_p[ free_i ].l -= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
                        *( N * )( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
                    *( P * )( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p - E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
                    E_base_S->E_mem_Q_blk_S_allocated[ table_i ].p -= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                    E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                    if( table_i == E_base_S->E_mem_Q_blk_S_table_allocated_id )
                        E_base_S->E_mem_Q_blk_S_allocated = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id++ ].p;
                    return 0;
                }
                break;
            }
        for_n_( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
            if( p_0 + l_0 == free_p[ free_i ].p )
            {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u )
                {   free_p[ free_i ].l -= E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                    if( free_p[ free_i ].l )
                        free_p[ free_i ].p += E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u;
                    else
                        free_p[ free_i ].p = 0;
                    if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
                        *( N * )( p_0 + l_0 + rel_addr_l ) = l;
                    *( P * )( p_0 + l_0 + rel_addr_p ) = p;
                    return E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n++;
                }
                break;
            }
    }
    Pc p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( table_i
    , E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u
    , E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n + 1
    , E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n ? p_0 : 0
    , l_0
    , 0
    , ~0
    );
    if( !p_1 )
        return ~0;
    if( table_i == E_mem_Q_blk_S_allocated_S_mapped_id
    || table_i == E_mem_Q_blk_S_allocated_S_free_id
    )
    {   *( P * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = 0;
        *( N * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = 0;
        if( !E_mem_Q_blk_Q_sys_table_mf_I_unite( table_i, rel_addr_p, rel_addr_l, p, l ))
        {   *( P * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
            *( N * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
        }
    }else
    {   *( P * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_p ) = p;
        if( table_i != E_base_S->E_mem_Q_blk_S_table_allocated_id )
            *( N * )( p_1 + ( E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1 ) * E_base_S->E_mem_Q_blk_S_allocated[ table_i ].u + rel_addr_l ) = l;
    }
    return E_base_S->E_mem_Q_blk_S_allocated[ table_i ].n - 1;
}
// Dla tablic systemowych “mapped” i “free” ‘alokuje’ tyle “n”, ile żądane, lub więcej.
_internal
P
E_mem_Q_blk_Q_table_M_from_free_or_map_0( N allocated_or_table_i
, N u
, N n
, P p // Adres uprzedniej zawartości lub 0, gdy brak.
, N l // I rozmiar. Jeśli “p == 0”, to parametr ignorowany.
, N l_rel
, N align
){  N l_1 = n * u;
    Pc p_1;
    if(n)
    {   if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id
        && p // Uprzedni obszar tablicy staje się wolnym blokiem.
        )
        {   n++;
            l_1 += u;
        }
        N l_align;
        if( allocated_or_table_i == E_base_S->E_mem_Q_blk_S_table_allocated_id
        || allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id
        || allocated_or_table_i == E_mem_Q_blk_S_allocated_S_mapped_id
        )
            l_align = sizeof(N);
        else if( ~align )
            l_align = align;
        else if( u > sizeof(N) && u % sizeof(N) == 0 )
            l_align = sizeof(N);
        else if( u <= sizeof(N) && E_simple_Z_n_T_power_2(u) )
            l_align = u;
        else
            l_align = 1;
        if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id ) // Obszar przed wyrównanym adresem staje się wolnym blokiem.
        {   n++;
            l_1 += u;
        }
        N l_ = ~0;
        N i_found;
        struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
        for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku na całą tablicę.
        {   p_1 = E_simple_Z_p_I_align_up_to_v2( free_p[ free_i ].p, l_align );
            if( free_p[ free_i ].l >= ( p_1 - free_p[ free_i ].p ) + l_1
            && free_p[ free_i ].l < l_
            )
            {   l_ = free_p[ free_i ].l;
                i_found = free_i;
                if( l_ == ( p_1 - free_p[ free_i ].p ) + l_1 )
                    break;
            }
        }
        if( !~l_ )
        {   // Jeśli żądanie ‘alokacji’ tablicy systemowej “mapped” lub “free” przyszło z rekurencji, to wtedy żądanie ‘alokacji’ takiej tablicy oznacza brak wolnych wpisów na poprzednią czekającą ‘alokację’ ogólną lub tablicy “allocated”, a za chwilę będą być może potrzebne jeszcze kolejne wpisy na bieżącą ‘alokację’. dlatego należy ‘alokować’ odrazu minimalną ilość rezerwowych wpisów (dla jednej tablicy, by nie wymuszać statycznie jednoczesnej ‘realokacji’ obu tablic), ponieważ w pesymistycznym przypadku ewentualne ‘doalokowanie’ jednej nadmiarowej strony pamięci na nie wykorzystane rezerwowe wpisy mniej kosztuje niż drugie kopiowanie tej samej tablicy dla ‘alokacji’ kolejnych wpisów w tej kolejnej rekurencji.
            if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id ) // Obszar przed wyrównanym adresem nie staje się wolnym blokiem.
            {   n--;
                l_1 -= u;
            }
            Pc src_page, src_page_end;
            if(p) // Będzie kopiowanie, a teraz przygotowanie do ‘remapowania’ ‚środka’ zamiast kopiowania.
            {   src_page = E_simple_Z_p_I_align_up_to_v2( (Pc)p, E_base_S->E_mem_S_page_size );
                src_page_end = E_simple_Z_p_I_align_down_to_v2( (Pc)p + l, E_base_S->E_mem_S_page_size );
                // Zapewnienie liczby wpisów w tablicy systemowej właśnie ‘realokowanej’— potrzebnej dla pesymistycznego przypadku niescalenia z już obecnymi nowych bloków dodawanych w tym wywołaniu procedury do takiej tablicy— by nie mogło wystąpić rekurencyjne, zapętlające wywołanie tej procedury dla tej samej tablicy bez możliwości uzyskania nowych wpisów.
                if( src_page < src_page_end ) // Będzie ‘remapowanie’ wewnętrznych “stron” pamięci (bloków o adresach wyrównanych do rozmiaru “strony”).
                {   if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id ) // Zostaną one wzięte z obszaru dostępnych wolnych bloków (‘przemapowane’), a krańcowe pozostałości (od obszaru wyrównanego do “stron” pamięci) staną się wolnymi blokami.
                    {   n--; // Nie będzie potrzeba osobno wpisywać uprzedniego rozmiaru tablicy.
                        l_1 -= u;
                        if( src_page != p ) // Będzie wolny początkowy fragment w pierwszej “stronie” pamięci: w źródłowym bloku i nowym.
                        {   n += 2;
                            l_1 += 2 * u;
                        }
                        if( src_page_end != (Pc)p + l ) // Będzie wolny końcowy fragment w ostatniej “stronie” pamięci: w źródłowym bloku i nowym.
                        {   n += 2;
                            l_1 += 2 * u;
                        }
                    }else if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_mapped_id ) // Zostaną one wzięte z obszaru ‘zmapowanych’, a krańcowe “strony” staną się osobnymi ‘zmapowanymi’ blokami.
                    {   n += 2;
                        l_1 += 2 * u;
                    }
                }else if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_mapped_id // Nowy blok, ‘zmapowany’ na tablicę ‘zmapowanych’— musi być wpisany do tablicy.
                || allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id // Fragment pozostały do wyrównania do rozmiaru “stron” pamięci staje się wolnym blokiem.
                )
                {   n++;
                    l_1 += u;
                }
            }else if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id ) // Fragment pozostały do wyrównania do rozmiaru “stron” pamięci staje się wolnym blokiem.
            {   n++;
                l_1 += u;
            }
            N dst_rel = 0;
            if( p
            && src_page < src_page_end
            )
            {   l_ = E_simple_Z_n_I_align_up_to_v2( l_1 - ( src_page - (Pc)p ), E_base_S->E_mem_S_page_size );
                l_ += dst_rel = E_simple_Z_n_I_align_up_to_v2( l_rel + ( src_page - (Pc)p ), E_base_S->E_mem_S_page_size );
            }else
                l_ = E_simple_Z_n_I_align_up_to_v2( l_1, E_base_S->E_mem_S_page_size );
                #if defined( __gnu_linux__ )
            if( E_base_S->E_flow_Z_task_stacks_S_n_pages < l_ / E_base_S->E_mem_S_page_size )
            {   N n_pages;
                V1_( n_pages = sysconf( _SC_AVPHYS_PAGES ));
                if( n_pages < l_ / E_base_S->E_mem_S_page_size )
                    return 0;
                E_base_S->E_flow_Z_task_stacks_S_n_pages = n_pages;
            }
            V1p( p_1 = mmap( 0
            , l_
            , PROT_READ | PROT_WRITE
            , MAP_PRIVATE
            | MAP_ANONYMOUS | MAP_UNINITIALIZED
            , -1
            , 0
            ))
                #elif defined( __FreeBSD__ ) || defined( __OpenBSD__ )
            V1p( p_1 = mmap( 0
            , l_
            , PROT_READ | PROT_WRITE
            , MAP_PRIVATE
            | MAP_ANON
            , -1
            , 0
            ))
                #else
#error not implemented
                #endif
                return 0;
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( p_1, l_, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            E_base_S->E_flow_Z_task_stacks_S_n_pages -= l_ / E_base_S->E_mem_S_page_size;
            N failed_tasks = E_flow_Q_task_I_granulation();
            if( failed_tasks )
            {   GV_(NDFN); Gd( -failed_tasks );
            }
            Pc dst_page;
            Pc p_ = p_1;
            if(p)
            {   if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                    E_flow_Q_task_I_touch_stack();
                //TODO Poniżej być może zastąpić (bez deintegracji) procedurą “move”, która powstanie.
                if( src_page < src_page_end )
                {   dst_page = p_1 + dst_rel;
                    p_ = dst_page - ( src_page - (Pc)p );
                    E_mem_Q_blk_I_copy( p_, p, src_page - (Pc)p );
                    p_ -= l_rel;
                    Pc src_page_ = src_page, dst_page_end = dst_page;
                    while( src_page_ != src_page_end )
                    {
                            #if defined( __gnu_linux__ )
                        V1p_( mremap( src_page_
                        , E_base_S->E_mem_S_page_size
                        , E_base_S->E_mem_S_page_size
                        , MREMAP_MAYMOVE | MREMAP_FIXED
                        , dst_page_end
                        ));
                            #elif defined( __FreeBSD__ ) || defined( __OpenBSD__ )
                        // Kopiowanie po jednej “stronie”, by tak właśnie zachodziło rzeczywiste ‘mapowanie’ w sprzętowym trybie “map on write”, gdy pojedyncze “strony” są też ‘odmapowywane’.
                        E_mem_Q_blk_I_copy( dst_page_end, src_page_, E_base_S->E_mem_S_page_size );
                        V0_( munmap( src_page_, E_base_S->E_mem_S_page_size ));
                            #else
#error not implemented
                            #endif
                        src_page_ += E_base_S->E_mem_S_page_size;
                        dst_page_end += E_base_S->E_mem_S_page_size;
                    }
                        #if 0 && defined( __gnu_linux__ )
#error nie działa ‘remapowanie’ całego bloku naraz, a tylko po “stronie”, jak powyżej.
                    V1p_( mremap( src_page
                    , src_page_end - src_page
                    , src_page_end - src_page
                    , MREMAP_MAYMOVE | MREMAP_FIXED
                    , dst_page
                    ));
                    Pc dst_page_end = dst_page + ( src_page_end - src_page );
                        #endif
                    E_mem_Q_blk_I_copy( dst_page_end, src_page_end, (Pc)p + l - src_page_end );
                }else
                    E_mem_Q_blk_I_copy( p_1 + l_rel, p, l );
                if( allocated_or_table_i == E_base_S->E_mem_Q_blk_S_table_allocated_id )
                    E_base_S->E_mem_Q_blk_S_allocated = (P)p_;
            }else
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].u = u;
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].p = p_;
            //TODO Wyeliminować “unite” tam, gdzie nigdy nie zajdzie.
            struct E_mem_Q_blk_Z_free free_p_;
            if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id )
            {   free_p = (P)p_;
                if( p
                && src_page < src_page_end
                )
                {   N i = 0;
                    if( src_page != p )
                    {   if( E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, src_page - (Pc)p ))
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = 0;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = 0;
                        }else
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = p;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = src_page - (Pc)p;
                        }
                        i++;
                        if( E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_1, p_ - p_1 ))
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = 0;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = 0;
                        }else
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = p_1;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = p_ - p_1;
                        }
                        i++;
                    }
                    if( src_page_end != (Pc)p + l )
                    {   if( E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, src_page_end, (Pc)p + l - src_page_end ))
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = 0;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = 0;
                        }else
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = src_page_end;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = (Pc)p + l - src_page_end;
                        }
                        i++;
                        if( E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_ + l_1, p_1 + l_ - ( p_ + l_1 )))
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = 0;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = 0;
                        }else
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].p = p_ + l_1;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + i ].l = p_1 + l_ - ( p_ + l_1 );
                        }
                    }
                }else
                {   if( l_ != l_1
                    && !E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_1 + l_1, l_ - l_1 )
                    )
                    {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = p_1 + l_1;
                        free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = l_ - l_1;
                    }else
                    {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = 0;
                        free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = 0;
                    }
                    if(p)
                        if( E_mem_Q_blk_Q_sys_table_mf_I_unite( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, l ))
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].p = 0;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].l = 0;
                        }else
                        {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].p = p;
                            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].l = l;
                        }
                }
            }else
            {   if( p
                && src_page < src_page_end
                )
                {   if( src_page != p )
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, src_page - (Pc)p ))
                            return 0;
                    if( p_ != p_1 )
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_1, p_ - p_1 ))
                            return 0;
                    if( src_page_end != (Pc)p + l )
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, src_page_end, (Pc)p + l - src_page_end ))
                            return 0;
                    if( p_1 + l_ != p_ + l_rel + l_1 )
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_ + l_rel + l_1, p_1 + l_ - ( p_ + l_rel + l_1 )))
                            return 0;
                }else
                {   if( l_ != l_1 )
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_1 + l_1, l_ - l_1 ))
                            return 0;
  	                  if(p)
                        if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, l ))
                            return 0;
                }
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_mapped_id )
            {   if( src_page < src_page_end )
                {   struct E_mem_Q_blk_Z_mapped *mapped_p = (P)p_;
                    for_n( i, E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n )
                    {   Pc p_end = mapped_p->p + mapped_p->l;
                        if( mapped_p->p <= (Pc)p
                        && (Pc)p < p_end
                        )
                        {   mapped_p->l = src_page - mapped_p->p;
                            if( mapped_p->p == p )
                                mapped_p->p = 0;
                            mapped_p = (P)p_;
                            if( (Pc)p + l != p_end )
                            {   mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = src_page_end;
                                mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = p_end - src_page_end;
                            }else
                            {   mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = 0;
                                mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = 0;
                            }
                            break;
                        }
                        mapped_p++;
                    }
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n++;
                }
                struct E_mem_Q_blk_Z_mapped *mapped_p = (P)p_;
                if( E_mem_Q_blk_Q_sys_table_mf_I_unite( allocated_or_table_i, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, p_1, l_ ))
                {   mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = 0;
                    mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = 0;
                }else
                {   mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = p_1;
                    mapped_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = l_;
                }
            }else
            {   if( p
                && src_page < src_page_end
                )
                {   struct E_mem_Q_blk_Z_mapped *mapped_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_mapped_id ].p;
                    for_n( i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_mapped_id ].n )
                    {   Pc p_end = mapped_p->p + mapped_p->l;
                        if( mapped_p->p <= (Pc)p
                        && (Pc)p < p_end
                        )
                        {   mapped_p->l = src_page - mapped_p->p;
                            if( mapped_p->p == p )
                                mapped_p->p = 0;
                            if( (Pc)p + l != p_end )
                                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, src_page_end, p_end - src_page_end ))
                                    return 0;
                            break;
                        }
                        mapped_p++;
                    }
                }
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, p_1, l_ ))
                    return 0;
            }
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n = n;
            return p_;
        }
        Pc old_free_p = free_p[ i_found ].p;
        p_1 = E_simple_Z_p_I_align_up_to_v2( old_free_p, l_align );
        free_p[ i_found ].l -= ( p_1 - old_free_p ) + l_1;
        if( free_p[ i_found ].l )
            free_p[ i_found ].p += ( p_1 - old_free_p ) + l_1;
        else
            free_p[ i_found ].p = 0;
        if(p)
        {   if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                E_flow_Q_task_I_touch_stack();
            E_mem_Q_blk_I_copy( p_1 + l_rel, p, l );
            if( allocated_or_table_i == E_base_S->E_mem_Q_blk_S_table_allocated_id )
                E_base_S->E_mem_Q_blk_S_allocated = (P)p_1;
        }
        if( p_1 - old_free_p )
            if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id )
            {   free_p = (P)( p_1 + l_rel );
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].p = old_free_p;
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].l = p_1 - old_free_p;
            }else
            {   struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, old_free_p, p_1 - old_free_p ))
                    return 0;
            }
        else if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id )
        {   free_p = (P)( p_1 + l_rel );
            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].p = 0;
            free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n + 1 ].l = 0;
        }
    }else if( !( p_1 = E_mem_Q_blk_R_new_0() ))
        return 0;
    if( !p ) //NDFN Rozpoznanie niebezpośrednie, mimo że jednoznaczne w obecnej implementacji.
        E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].u = u;
    E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].p = p_1;
    if(p)
    {   struct E_mem_Q_blk_Z_free free_p_;
        if( allocated_or_table_i == E_mem_Q_blk_S_allocated_S_free_id )
        {   struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].p;
            if( E_mem_Q_blk_Q_sys_table_mf_I_unite( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, l ))
            {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = 0;
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = 0;
            }else
            {   free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].p = p;
                free_p[ E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n ].l = l;
            }
        }else
        {   if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, l ))
                return 0;
        }
    }
    E_base_S->E_mem_Q_blk_S_allocated[ allocated_or_table_i ].n = n; // Końcowe wpisy i tak nie zawierałyby danych dla powyższego “unite”, a “E_mem_Q_blk_S_allocated_S_mapped_id” może wystąpić tylko w końcowym wywołaniu rekurencyjnym (pojedynczy poziom rekurencji), nie w odzewnętrznym, początkowym wywołaniu tej procedury, więc powiększenie tablicy dopiero tutaj.
    return p_1;
}
_internal
__attribute__ ((__malloc__))
P
E_mem_Q_blk_R_new_0( void
){  Pc p_end;
    V1p_( p_end = sbrk(0) );
    Pc p = (P)1;
    B exists;
    O{  exists = no;
        N allocated_i = E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n;
        while( allocated_i-- )
        {   if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == p )
            {   exists = yes;
                break;
            }
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p < p_end
            && E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p > p
            )
            {   p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                exists = yes;
                break;
            }
        }
        if( !exists )
            return p;
        if( ++p == p_end )
            break;
    }
    return 0;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
P
E_mem_Q_blk_M( N l
){  return E_mem_Q_blk_M_tab( 1, l );
}
P
E_mem_Q_blk_M_tab(
  N u
, N n
){  return E_mem_Q_blk_M_align_tab( u, n, ~0 );
}
P
E_mem_Q_blk_M_align( N l
, N align
){  return E_mem_Q_blk_M_align_tab( 1, l, align );
}
__attribute__ ((__malloc__))
P
E_mem_Q_blk_M_align_tab(
  N u
, N n
, N align
){  J_assert(u);
    J_assert( !E_simple_T_multiply_overflow( n, u ));
    J_assert(align);
    _single_thread_begin;
    struct E_mem_Q_blk_Z_allocated allocated_p;
    N allocated_i = E_mem_Q_blk_Q_sys_table_R_new_id( E_base_S->E_mem_Q_blk_S_table_allocated_id, (Pc)&allocated_p.p - (Pc)&allocated_p, (Pc)&allocated_p.n - (Pc)&allocated_p, 0, 0 );
    if( !~allocated_i )
    {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
        _single_thread_end;
        return 0;
    }
    struct E_mem_Q_blk_Z_mapped mapped_p_;
    struct E_mem_Q_blk_Z_free free_p_;
    if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
    || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
    || !E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i, u, n, 0, 0, 0, align )
    )
    {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
        _single_thread_end;
        return 0;
    }
        #ifdef E_mem_Q_blk_C_debug
    E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
        #endif
    P p_ = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    return p_;
}
P
E_mem_Q_blk_M_replace( P p
, N l
){  return E_mem_Q_blk_M_replace_tab( p, 1, l );
}
__attribute__ ((__malloc__))
P
E_mem_Q_blk_M_replace_tab( P *p
, N u
, N n
){  J_assert(u);
    J_assert( !E_simple_T_multiply_overflow( n, u ));
    _single_thread_begin;
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   if( u * n == E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
            {   _single_thread_end;
                return *p;
            }
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, *( P * )p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u ))
                {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
                    _single_thread_end;
                    return 0;
                }
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            || !E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i, u, n, 0, 0, 0, ~0 )
            )
            {   *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = 0;
                E_mem_Q_blk_I_assert_on_return( __LINE__ );
                _single_thread_end;
                return 0;
            }
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            P p_ = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
            E_mem_Q_blk_I_assert_on_return( __LINE__ );
            _single_thread_end;
            *( P * )p = p_;
            return p_;
        }
    _single_thread_end;
    GV_(NXP); Gh(p); V();
}
__attribute__ ((__malloc__))
P
E_mem_Q_blk_M_split( P p
, N i
){  J_assert(i);
    _single_thread_begin;
    struct E_mem_Q_blk_Z_allocated allocated_p;
    N allocated_i = E_mem_Q_blk_Q_sys_table_R_new_id( E_base_S->E_mem_Q_blk_S_table_allocated_id, (Pc)&allocated_p.p - (Pc)&allocated_p, (Pc)&allocated_p.n - (Pc)&allocated_p, 0, 0 );
    if( !~allocated_i )
    {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
        _single_thread_end;
        return 0;
    }
    for_n( allocated_j, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].p == p )
        {   J_assert( i < E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].n );
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n = E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].n - i;
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].n = i;
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u = E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].u;
            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = (Pc)p + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_j ].u;
            P p_ = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
            E_mem_Q_blk_I_assert_on_return( __LINE__ );
            _single_thread_end;
            return p_;
        }
    _single_thread_end;
    GV_(NXP); Gh(p); V();
}
N
E_mem_Q_blk_W( P p
){  if( U_R( E_base_S->E_flow_S_signal, exit_all )) //NDFN To sprawdzenie raczej umieszczać tylko w funkcjach wysokopoziomowych.
        return 0;
    _single_thread_begin;
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == p )
        {   E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = 0;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u ))
                {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
                    _single_thread_end;
                    return ~0;
                }
            }
            E_mem_Q_blk_I_assert_on_return( __LINE__ );
            _single_thread_end;
            return 0;
        }
    _single_thread_end;
    GV_(NXP); Gh(p); V();
}
//------------------------------------------------------------------------------
_internal
P
E_mem_Q_blk_I_add_( P p
, N n
, N *n_prepended
, N *n_appended
){  J_assert(n); // Puste użycie tej procedury.
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   Pc p_0 = 0;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            N l = n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   N l_1 = 0;
                p_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
                for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                    if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
                    {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
                        {   l_1 = free_p[ free_i ].l;
                            if( l_1 > l )
                                l_1 = l;
                            else if( l_1 < l )
                                l_1 = E_simple_Z_n_I_align_down_to_v( l_1, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
                            l -= l_1;
                        }
                        break;
                    }
                if( !l )
                {   free_p[ free_i ].l -= l_1;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                    *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                        #ifdef E_mem_Q_blk_C_debug
                    E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                        #endif
                    if( n_prepended )
                        *n_prepended = n;
                    if( n_appended )
                        *n_appended = 0;
                    return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1;
                }
                for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
                    if( p_0 + l_0 == free_p[ free_j ].p )
                    {   if( free_p[ free_j ].l >= l )
                        {   free_p[ free_j ].l -= l;
                            if( free_p[ free_j ].l )
                                free_p[ free_j ].p += l;
                            else
                                free_p[ free_j ].p = 0;
                            if( l_1 )
                            {   free_p[ free_i ].l -= l_1;
                                if( !free_p[ free_i ].l )
                                    free_p[ free_i ].p = 0;
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                            }
                            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                                #ifdef E_mem_Q_blk_C_debug
                            E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                            E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1 + l_0, l, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                                #endif
                            if( n_prepended )
                                *n_prepended = l_1 / E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                            if( n_appended )
                                *n_appended = l / E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1 + l_0;
                        }
                        break;
                    }
                l += l_1; // Przywraca oryginalną wartość sprzed scalenia od dołu, skoro był blok przyległy od dołu, zabrakło do pełnej liczby od góry.
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            )
                return 0;
            P p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n + n
            , p_0
            , l_0
            , 0
            , ~0
            );
            if( !p_1 )
                return 0;
            *( P * )p = p_1;
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( (Pc)p_1 + l_0, l, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            if( n_prepended )
                *n_prepended = 0;
            if( n_appended )
                *n_appended = n;
            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_0;
        }
    return (P)~0;
}
_internal
P
E_mem_Q_blk_I_prepend_append_( P p
, N n_prepend
, N n_append
){  J_assert( n_prepend || n_append ); // Puste użycie tej procedury.
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   Pc p_0 = 0;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   N l = ( n_prepend + n_append ) * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                N l_1 = 0;
                p_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
                for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                    if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
                    {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
                        {   l_1 = free_p[ free_i ].l;
                            if( l_1 > l )
                                l_1 = l;
                            else if( l_1 < l )
                                l_1 = E_simple_Z_n_I_align_down_to_v( l_1, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
                            l -= l_1;
                        }
                        break;
                    }
                if( !n_append
                && !l
                )
                {   free_p[ free_i ].l -= l_1;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n_prepend;
                    return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1;
                }
                // Kontynuacja od tej linii: trzeba wybrać optymalne przesunięcie pomiędzy blokiem poprzedzającym a następującym, by ewentualnie nie kopiować.
                for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
                    if( p_0 + l_0 == free_p[ free_j ].p )
                    {   if( free_p[ free_j ].l >= l )
                        {   free_p[ free_j ].l -= l;
                            if( free_p[ free_j ].l )
                                free_p[ free_j ].p += l;
                            else
                                free_p[ free_j ].p = 0;
                            if( l_1 )
                            {   free_p[ free_i ].l -= l_1;
                                if( !free_p[ free_i ].l )
                                    free_p[ free_i ].p = 0;
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                            }
                            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n_prepend + n_append;

                            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1 + l_0;
                        }
                        break;
                    }
                l += l_1; // Przywraca oryginalną wartość sprzed scalenia od dołu, skoro był blok przyległy od dołu, zabrakło do pełnej liczby od góry.
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            )
                return 0;
            P p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n + n_prepend + n_append
            , p_0
            , l_0
            , n_prepend * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , ~0
            );
            if( !p_1 )
                return 0;
            *( P * )p = p_1;
            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
        }
    return (P)~0;
}
_internal
P
E_mem_Q_blk_I_append_( P p
, N n
, N align
){  J_assert(n); // Puste użycie tej procedury.
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   N l = n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            Pc p_0 = 0;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   N l_1 = 0;
                p_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
                for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
                    if( p_0 + l_0 == free_p[ free_i ].p )
                    {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
                        {   l_1 = free_p[ free_i ].l;
                            if( l_1 > l )
                                l_1 = l;
                            else if( l_1 < l )
                                l_1 = E_simple_Z_n_I_align_down_to_v( l_1, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
                            l -= l_1;
                        }
                        break;
                    }
                if( !l
                && ( !~align
                  || E_simple_Z_p_T_aligned_to_v2( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, align )
                ))
                {   free_p[ free_i ].l -= l_1;
                    if( free_p[ free_i ].l )
                        free_p[ free_i ].p += l_1;
                    else
                        free_p[ free_i ].p = 0;
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                        #ifdef E_mem_Q_blk_C_debug
                    E_mem_Q_blk_P_fill_c( p_0 + l_0, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                        #endif
                    return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_0;
                }
                if( ~align )
                {   for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                    {   if( free_p[ free_j ].p + free_p[ free_j ].l == p_0 )
                        {   Pc p_align = E_simple_Z_p_I_align_up_to_v2( free_p[ free_j ].p, align );
                            if( free_p[ free_j ].l >= ( p_align - free_p[ free_j ].p ) + l )
                            {   free_p[ free_j ].l = p_align - free_p[ free_j ].p;
                                if( !free_p[ free_j ].l )
                                    free_p[ free_j ].p = 0;
                                if( l_1 )
                                {   free_p[ free_i ].l -= l_1;
                                    free_p[ free_i ].l += ( p_0 - p_align ) - l;
                                    if( free_p[ free_i ].l )
                                    {   free_p[ free_i ].p += l_1;
                                        free_p[ free_i ].p -= ( p_0 - p_align ) - l;
                                    }else
                                        free_p[ free_i ].p = 0;
                                }else if(( p_0 - p_align ) - l )
                                {   struct E_mem_Q_blk_Z_free free_p_;
                                    if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p_align + l_0 + l, ( p_0 - p_align ) - l ))
                                        E_mem_Q_blk_I_assert_on_return( __LINE__ );
                                }
                                if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                                    E_flow_Q_task_I_touch_stack();
                                E_mem_Q_blk_I_copy( p_align, p_0, l_0 );
                                    #ifdef E_mem_Q_blk_C_debug
                                E_mem_Q_blk_P_fill_c( p_align + l_0, l + l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                                    #endif
                                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = p_align;
                                return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_0;
                            }
                            break;
                        }
                    }
                }else
                {   for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                        if( free_p[ free_j ].p + free_p[ free_j ].l == p_0 )
                        {   if( free_p[ free_j ].l >= l )
                            {   if( free_p[ free_j ].l == l )
                                    free_p[ free_j ].p = 0;
                                free_p[ free_j ].l -= l;
                                if( l_1 )
                                {   free_p[ free_i ].l -= l_1;
                                    if( free_p[ free_i ].l )
                                        free_p[ free_i ].p += l_1;
                                    else
                                        free_p[ free_i ].p = 0;
                                }
                                Pc p_1 = p_0 - l;
                                if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                                    E_flow_Q_task_I_touch_stack();
                                E_mem_Q_blk_I_copy( p_1, p_0, l_0 );
                                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                                    #ifdef E_mem_Q_blk_C_debug
                                E_mem_Q_blk_P_fill_c( p_1 + l_0, l + l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                                    #endif
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = p_1;
                                return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_0;
                            }
                            break;
                        }
                }
                l += l_1; // Przywraca oryginalną wartość sprzed scalenia od dołu, skoro był blok przyległy od dołu, a zabrakło do pełnej liczby od góry.
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            )
                return 0;
            P p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n + n
            , p_0
            , l_0
            , 0
            , align
            );
            if( !p_1 )
                return 0;
            *( P * )p = p_1;
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( (Pc)p_1 + l_0, l, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            return (Pc)p_1 + l_0;
        }
    return (P)~0;
}
_internal
P
E_mem_Q_blk_I_prepend_( P p
, N n
){  J_assert(n); // Puste użycie tej procedury.
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   N l = n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            Pc p_0 = 0;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   N l_1 = 0;
                p_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
                for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                    if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
                    {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
                        {   l_1 = free_p[ free_i ].l;
                            if( l_1 > l )
                                l_1 = l;
                            else if( l_1 < l )
                                l_1 = E_simple_Z_n_I_align_down_to_v( l_1, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
                            l -= l_1;
                        }
                        break;
                    }
                if( !l )
                {   free_p[ free_i ].l -= l_1;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                    *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                        #ifdef E_mem_Q_blk_C_debug
                    E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                        #endif
                    return p_0;
                }
                for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
                    if( p_0 + l_0 == free_p[ free_j ].p )
                    {   if( free_p[ free_j ].l >= l )
                        {   free_p[ free_j ].l -= l;
                            if( free_p[ free_j ].l )
                                free_p[ free_j ].p += l;
                            else
                                free_p[ free_j ].p = 0;
                            if( l_1 )
                            {   free_p[ free_i ].l -= l_1;
                                if( !free_p[ free_i ].l )
                                    free_p[ free_i ].p = 0;
                            }
                            if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                                E_flow_Q_task_I_touch_stack();
                            E_mem_Q_blk_I_copy_rev( p_0 + l, p_0, l_0 );
                            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                            if( l_1 )
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                                #ifdef E_mem_Q_blk_C_debug
                            E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                                #endif
                            return p_0 + l;
                        }
                        break;
                    }
                l += l_1; // Przywraca oryginalną wartość sprzed scalenia od dołu, skoro był blok przyległy od dołu, a zabrakło do pełnej liczby od góry.
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            )
                return 0;
            P p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n + n
            , p_0
            , l_0
            , l
            , ~0
            );
            if( !p_1 )
                return 0;
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( p_1, l, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            *( P * )p = p_1;
            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l;
        }
    return (P)~0;
}
_internal
P
E_mem_Q_blk_I_insert_( P p
, N i
, N n
){  J_assert(n);
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   J_assert( i <= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n );
            Pc p_0 = 0;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   N l = n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                N l_1 = 0;
                p_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
                struct E_mem_Q_blk_Z_free *free_p = (P)E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].p;
                for_n( free_i, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od dołu.
                    if( free_p[ free_i ].p + free_p[ free_i ].l == p_0 )
                    {   if( free_p[ free_i ].l >= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u )
                        {   l_1 = free_p[ free_i ].l;
                            if( l_1 > l )
                                l_1 = l;
                            else if( l_1 < l )
                                l_1 = E_simple_Z_n_I_align_down_to_v( l_1, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
                            l -= l_1;
                        }
                        break;
                    }
                if( !l )
                {   free_p[ free_i ].l -= l_1;
                    if( !free_p[ free_i ].l )
                        free_p[ free_i ].p = 0;
                    E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                    *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                    if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                        E_flow_Q_task_I_touch_stack();
                    E_mem_Q_blk_I_copy( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p
                    , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1
                    , i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                    );
                        #ifdef E_mem_Q_blk_C_debug
                    E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, l_1, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                        #endif
                    return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                }
                for_n( free_j, E_base_S->E_mem_Q_blk_S_allocated[ E_mem_Q_blk_S_allocated_S_free_id ].n ) // Szukanie wolnego bloku przyległego od góry.
                    if( p_0 + l_0 == free_p[ free_j ].p )
                    {   if( free_p[ free_j ].l >= l )
                        {   free_p[ free_j ].l -= l;
                            if( free_p[ free_j ].l )
                                free_p[ free_j ].p += l;
                            else
                                free_p[ free_j ].p = 0;
                            if( l_1 )
                            {   free_p[ free_i ].l -= l_1;
                                if( !free_p[ free_i ].l )
                                    free_p[ free_i ].p = 0;
                                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p -= l_1;
                            }
                            E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n += n;
                            if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                                E_flow_Q_task_I_touch_stack();
                            if( l_1 )
                                E_mem_Q_blk_I_copy( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p
                                , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1
                                , i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                                );
                            E_mem_Q_blk_I_copy_rev( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1 + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u + l
                            ,  E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + l_1 + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                            , l_0 - i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                            );
                                #ifdef E_mem_Q_blk_C_debug
                            E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, l_1 + l, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                                #endif
                            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                        }
                        break;
                    }
            }
            struct E_mem_Q_blk_Z_mapped mapped_p_;
            struct E_mem_Q_blk_Z_free free_p_;
            if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
            || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
            )
                return 0;
            P p_1 = E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
            , E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n + n
            , p_0
            , l_0
            , 0
            , ~0
            );
            if( !p_1 )
                return 0;
            //TODO Zrobić w “E_mem_Q_blk_Q_table_M_from_free_or_map” parametr przesuniecia dla ‘split’ i kopiowania tam odrazu?
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
                E_mem_Q_blk_I_copy_rev( (Pc)p_1 + ( i + n ) * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                , (Pc)p_1 + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                , l_0 - i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u
                ); // “E_flow_Q_task_I_touch_stack” było wykonane w “E_mem_Q_blk_Q_table_M_from_free_or_map”.
                #ifdef E_mem_Q_blk_C_debug
            E_mem_Q_blk_P_fill_c( (Pc)p_1 + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
                #endif
            *( P * )p = p_1;
            return (Pc)p_1 + i * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
        }
    return (P)~0;
}
_internal
P
E_mem_Q_blk_I_remove_( P p
, N i
, N n
){  J_assert(n);
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == *( P * )p )
        {   J_assert( i + n <= E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n );
            N l = n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n == n ) // Usuwany cały blok.
            {   struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_
                , *( P * )p
                , l
                ))
                    return 0;
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n = 0;
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = E_mem_Q_blk_R_new_0();
                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
            }else if( i + n == E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n ) // Usuwane na końcu bloku.
            {   struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_
                , *( Pc * )p + l_0 - l
                , l
                ))
                    return 0;
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n -= n;
            }else if( !i ) // Usuwane na początku bloku.
            {   P p_ = *( P * )p;
                struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_
                , p_
                , l
                ))
                    return 0;
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n -= n;
                *( P * )p = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p += l;
            }else // Usuwane w środku bloku.
            {   Pc p_0 = *( Pc * )p + ( i + n ) * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
                if( !U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read ))
                    E_flow_Q_task_I_touch_stack();
                E_mem_Q_blk_I_copy( p_0 - l
                , p_0
                , *( Pc * )p + l_0 - p_0
                );
                struct E_mem_Q_blk_Z_free free_p_;
                if( !~E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_
                , *( Pc * )p + l_0 - l
                , l
                ))
                    return 0;
                E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n -= n;
            }
            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
        }
    return (P)~0;
}
//------------------------------------------------------------------------------
// Resize useing a boundary without memory move if possible.
// Gdy wynik jest niezerowy, to:
// • jeżeli “*n_appended”, to w wyniku jest adres nowego bloku przyległego od góry.
// • jeżeli “!*n_appended”, 〃 adres podanego bloku (przed którym dołączono blok przyległy od dołu).
// Te wartości nie są używane i mogą zostać zmienione.
P
E_mem_Q_blk_I_add( P p
, N n
, N *n_prepended
, N *n_appended
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_add_( p, n, n_prepended, n_appended );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p_ )
    {   GV_(NXP); Gh( p_ ); V();
    }
    return p_;
}
// Resize useing boundaries without memory move if possible.
//NDFN Funkcja nie dokończona.
P
E_mem_Q_blk_I_prepend_append( P p
, N n_prepend
, N n_append
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_prepend_append_( p, n_prepend, n_append );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p_ )
    {   GV_(NXP); Gh( p_ ); V();
    }
    return p_;
}
// Resize useing high boundary without memory move if possible.
// W wyniku podaje adres dołączonego bloku.
P
E_mem_Q_blk_I_append( P p
, N n
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_append_( p, n, ~0 );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p_ )
    {   GV_(NXP); V();
    }
    return p_;
}
// Resize useing low boundary without memory move if possible.
// W wyniku podaje adres danych pierwotnego bloku.
P
E_mem_Q_blk_I_prepend( P p
, N n
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_prepend_( p, n );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p_ )
    {   GV_(NXP); V();
    }
    return p_;
}
// Insert without memory move if possible.
// W wyniku podaje adres wstawionego bloku.
P
E_mem_Q_blk_I_insert( P p
, N i
, N n
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_insert_( p, i, n );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p_ )
    {   GV_(NXP); Gh( p_ ); V();
    }
    return p_;
}
// Remove without memory move.
// W wyniku podaje nowy adres bloku (jak w “*( P * )p”).
P
E_mem_Q_blk_I_remove( P p
, N i
, N n
){  _single_thread_begin;
    P p_ = E_mem_Q_blk_I_remove_( p, i, n );
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    if( !~(N)p )
    {   GV_(NXP); V();
    }
    return p_;
}
//==============================================================================
    #ifdef C_to_libs_C_replace_c_alloc
size_t
        #if defined( __gnu_linux__ ) || defined( __OpenBSD__ )
malloc_usable_size( P p
        #elif defined( __FreeBSD__ )
malloc_usable_size( const void *p
        #endif
){  Dh_();
    _single_thread_begin;
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == p )
        {   _single_thread_end;
            return E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
        }
    _single_thread_end;
    GV_(NXP); Gh(p);
    return 0;
}
int
posix_memalign( P *p
, size_t align
, size_t size
){  Dh_();
    if( !E_simple_Z_n_T_power_2(align)
    || align % sizeof(P)
    )
        return EINVAL;
    P p_ = E_mem_Q_blk_M_malloc( size, align );
    if( !p_ )
        return ENOMEM;
    *p = p_;
    return 0;
}
_internal
P
E_mem_Q_blk_M_malloc( N l
, N align
){  return E_mem_Q_blk_M_tab_calloc( 1, l, align );
}
_internal
P
E_mem_Q_blk_M_tab_calloc(
  N u
, N n
, N align
){  _single_thread_begin;
    struct E_mem_Q_blk_Z_allocated allocated_p;
    N allocated_i = E_mem_Q_blk_Q_sys_table_R_new_id( E_base_S->E_mem_Q_blk_S_table_allocated_id, (Pc)&allocated_p.p - (Pc)&allocated_p, (Pc)&allocated_p.n - (Pc)&allocated_p, 0, 0 );
    if( !~allocated_i )
    {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
        _single_thread_end;
        return 0;
    }
    struct E_mem_Q_blk_Z_mapped mapped_p_;
    struct E_mem_Q_blk_Z_free free_p_;
    if( !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_mapped_id, (Pc)&mapped_p_.p - (Pc)&mapped_p_, (Pc)&mapped_p_.l - (Pc)&mapped_p_, 0, 0 )
    || !~E_mem_Q_blk_Q_sys_table_R_new_id( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, 0, 0 )
    || !E_mem_Q_blk_Q_table_M_from_free_or_map_0( allocated_i, u, n, 0, 0, 0, align )
    )
    {   E_mem_Q_blk_I_assert_on_return( __LINE__ );
        _single_thread_end;
        return 0;
    }
        #ifdef E_mem_Q_blk_C_debug
    E_mem_Q_blk_P_fill_c( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u, 0xa5 ); //TODO Procedura usuwająca zerowanie nowej pamięci na czas usuwania zmiennych globalnych i testów.
        #endif
    P p_ = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p;
    E_mem_Q_blk_I_assert_on_return( __LINE__ );
    _single_thread_end;
    return p_;
}
P
malloc( size_t l
){  Dh_();
    P p = E_mem_Q_blk_M_malloc( l, E_mem_Q_blk_S_align_to_all );
    if( !p )
        _errno = ENOMEM;
    return p;
}
P
calloc( size_t n
, size_t u
){  Dh_();
    if( E_simple_T_multiply_overflow( n, u ))
    {   _errno = ENOMEM;
        return 0;
    }
    P p = E_mem_Q_blk_M_tab_calloc( 1, n * u, E_mem_Q_blk_S_align_to_all );
    if( !p )
        _errno = ENOMEM;
    else
        _0( p, n * u );
    return p;
}
P
realloc( P p
, size_t l
){  Dh_();
    if( !p )
        return malloc(l);
    _single_thread_begin;
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == p )
        {   N l_0 = E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u;
            if( !l )
            {   E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = 0;
                if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
                {   struct E_mem_Q_blk_Z_free free_p_;
                    E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_
                    , p
                    , l_0
                    );
                }
                _single_thread_end;
                return 0;
            }
            if( l > E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   P p_ = E_mem_Q_blk_I_append_( &p, l - l_0, E_mem_Q_blk_S_align_to_all );
                _single_thread_end;
                if( p_ )
                    p_ = p;
                else
                    _errno = ENOMEM;
                return p_;
            }
            if( l < E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   P p_ = E_mem_Q_blk_I_remove_( &p, l, l_0 - l );
                _single_thread_end;
                return p_;
            }
            _single_thread_end;
            return p;
        }
    _single_thread_end;
    GV_(NXP); Gh(p);
    P p_;
    int errno_;
    Vpe((( p_ = ( *E_base_S->E_mem_Q_blk_I_libc_realloc )( p, l )) || !_errno ), errno_ )
        _errno = errno_;
    return p_;
}
void
free( P p
){  Dh_();
    if( !p ) // Funkcja zerowego adresu, taka jak w oryginalnym “free”.
        return;
    _single_thread_begin;
    for_n( allocated_i, E_base_S->E_mem_Q_blk_S_allocated[ E_base_S->E_mem_Q_blk_S_table_allocated_id ].n )
        if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p == p )
        {   E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].p = 0;
            if( E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n )
            {   struct E_mem_Q_blk_Z_free free_p_;
                E_mem_Q_blk_Q_sys_table_mf_P_put( E_mem_Q_blk_S_allocated_S_free_id, (Pc)&free_p_.p - (Pc)&free_p_, (Pc)&free_p_.l - (Pc)&free_p_, p, E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].n * E_base_S->E_mem_Q_blk_S_allocated[ allocated_i ].u );
            }
            _single_thread_end;
            return;
        }
    _single_thread_end;
    GV_(NXP); Gh(p);
    ( *E_base_S->E_mem_Q_blk_I_libc_free )(p);
}
    #endif
/******************************************************************************/
